%% Template for a scientific paper by Samuel Pawel
%% Last modification: 17. December 2020
\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphics}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{textcomp}
\usepackage[sc]{mathpazo}
\usepackage{times}
\usepackage{doi} % automatic doi-links
\usepackage[round]{natbib} % bibliography
\usepackage{multirow} % multi rows and columns for tables
\usepackage{longtable} % tables that span several pages
\usepackage{booktabs} % nicer tables
\usepackage[toc, page]{appendix} % better appendices
\usepackage{nameref} % reference appendices with names
\usepackage{todonotes}

%% margins
%% ----------------------------------------------------------------------------
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 right=25mm,
 top=20mm,
 bottom=20mm,
 }

%% title, authors, affiliations, mail
%% ----------------------------------------------------------------------------
\newcommand\longtitle{Pitfalls and Potentials in Simulation Studies}
\newcommand\shorttitle{\longtitle} % if longtitle too long, change here
% \newcommand\subtitle{How undisclosed flexibility in simulation experiments
% allows to show superiority of any method}
\newcommand\subtitle{Questionable research practices in %Undisclosed flexibility in design, execution, and reporting of
comparative simulation studies allows to show superiority of any method}
\newcommand\longauthors{Samuel Pawel, Lucas Kook}
\newcommand\shortauthors{\longauthors} % if longauthors too long, change here
\newcommand\affiliation{
  Epidemiology, Biostatistics and Prevention Institute (EBPI) \\
  Center for Reproducible Science (CRS) \\
  University of Zurich
}
\newcommand\mail{samuel.pawel@uzh.ch}
\title{
  \vspace{-2em}
  \textbf{\longtitle} \\
  \subtitle
}
\author{
  \textbf{\longauthors} \\
  \affiliation \\
  E-mail: \href{mailto:\mail}{\mail}
}
\date{\today} % don't forget to hard-code date when submitting to arXiv!

%% hyperref options
%% ----------------------------------------------------------------------------
\usepackage{hyperref}  
\hypersetup{
  bookmarksopen=true, 
  breaklinks=true,
  pdftitle={\shorttitle}, 
  pdfauthor={\shortauthors},
  pdfsubject={},
  pdfkeywords={},
  colorlinks=true,
  linkcolor=RoyalPurple,
  anchorcolor=black,
  citecolor=MidnightBlue,
  urlcolor=BrickRed,
}

%% Headers and footers
%% ----------------------------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\shorttitle}
\rhead{\shortauthors}

%% Useful commands
%% ----------------------------------------------------------------------------
%% Shortforms
\newcommand{\eg}{\textit{e.\,g.\,}} % e.g.
\newcommand{\ie}{\textit{i.\,e.\,}} % i.e.
\newcommand{\cf}{\textit{cf\,}} % cf
%% Distributions
\DeclareMathOperator{\Ber}{B} % Bernoulli
\DeclareMathOperator{\Bin}{Bin} % Binomial Distribution
\DeclareMathOperator{\Cauchy}{C} % Cauchy Distribution
\DeclareMathOperator{\Po}{Po} % Poisson
\DeclareMathOperator{\Exp}{Exp} % Exponential
\DeclareMathOperator{\Nor}{N} % Normal
\DeclareMathOperator{\stud}{t} % Student
\DeclareMathOperator{\Ga}{G} % Gamma
\DeclareMathOperator{\Be}{Be} % Beta
%% Operators and special functions
\DeclareMathOperator{\Var}{Var} % Variance
\DeclareMathOperator{\E}{\mathsf{E}} % Expectation
\DeclareMathOperator{\Cov}{Cov} % Covariance
\DeclareMathOperator{\Corr}{Corr} % Correlation 
\DeclareMathOperator{\se}{se} % Standard error
\DeclareMathOperator{\sign}{sign} % Sign
\DeclareMathOperator{\logit}{logit} % Logit
\DeclareMathOperator{\Mod}{Mod} % Modus
\DeclareMathOperator{\Med}{Med} % Median
\DeclareMathOperator{\diag}{diag} % Diagonalmatrix
\DeclareMathOperator{\trace}{tr} % Trace
\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Probability
\newcommand{\p}{f} % Density function
\newcommand{\B}{\operatorname{{B}}} % Beta function
\newcommand{\Lik}{L} % Likelihood function
\DeclareMathOperator{\arctanh}{arctanh} % Arcus tangens hyperbolicus
\DeclareMathOperator*{\argmax}{arg\,max} % Argmax
\DeclareMathOperator*{\argmin}{arg\,min} % Argmin
%% Other math things
\newcommand{\C}{\mathbb{C}} % Complex
\newcommand{\R}{\mathbb{R}} % Reals
\newcommand{\Q}{\mathbb{Q}} % Rationals
\newcommand{\Z}{\mathbb{Z}} % Integers
\newcommand{\N}{\mathbb{N}} % Naturals
\newcommand{\given}{\,\vert\,} % Given
\newcommand{\abs}[1]{\left\lvert#1\right\rvert} % Absolutvalue
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % Norm
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil} % Ceiling
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor} % Floor
\newcommand{\sprod}[1]{\left\langle#1\right\rangle} % Scalarproduct
\newcommand{\Ind}[2]{\mathsf{I}_{#2}(#1)} % Indicatorfunction
\newcommand{\IdMat}{\boldsymbol{\mathrm{I}}} % Identity matrix
\DeclareMathOperator{\BF}{BF} % Bayes factor
\newcommand\BFsub[2]{\BF_{\scriptscriptstyle{#1:#2}}} % BF with subscript
\newcommand{\that}{\hat{\theta}} % Effect estimate
\newcommand{\lw}[1]{W_{\scriptscriptstyle{#1}}} % Lambert W function
%% ----------------------------------------------------------------------------

\begin{document}
\maketitle

% knitr options
% ======================================================================
<< "main-setup", include = FALSE >>=
library(knitr)
opts_chunk$set(fig.height = 4,
               echo = FALSE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE, 
               eval = TRUE)
@

% Abstract
% ======================================================================
\begin{center}
\begin{minipage}{13cm}
{\small
\rule{\textwidth}{0.5pt} \\
{\centering \textbf{Abstract} \\
% Statisticians and methodologists often emphasize methodological flaws 
% in research from other scientific disciplines, yet the methodological 
% standards of empirical research in their own field are themeselves poor: 
Comparative simulation studies have become a workhorse tool for benchmarking
% operating characteristics 
of statistical methods. But if done in nontransparent and uncritical manner, they
may lead to over-optimism. Furthermore, the current publication requirements of 
statistics journals cannot prevent questionable research practices
such as selective reporting of results which favour a particular method.
The past years have witnessed numerous suggestions and initiatives to 
improve on these issues, most notably the need for simulation protocols, 
but only little progress can be seen to date. 
% In this paper we illustrate with an empirical study how easy it is to fool
% others (and ourselves) using poor simulation study methodology. 
In this paper we list common questionable research practices which undermine
the validity of findings from comparative simulation studies. We illustrate with 
an empirical study how easy it is to fool
others (and ourselves) using poor simulation study methodology.
We also use the same example to show how over-optimism can be avoided if a sound 
methodological approach is adopted instead.
Finally, we provide simple and concrete suggestions for researchers, reviewers,
and other academic stakeholders to improve the methodological quality of 
comparative simulation studies.
% with minimal impact on the publication 
% process.
}
\rule{\textwidth}{0.4pt} \\
\textit{Key words}: 
Simulation studies, transparency, over-optimism, questionable research practices
}
\end{minipage}
\end{center}

% Jelizarow, M., Guillemot, V., Tenenhaus, A., Strimmer, K. and Boulesteix, A. L. (2010) Over-optimism in bioinformatics: An illustration. Bioinformatics, 
% --> about data sets, this article about simulation

% Introduction
% ======================================================================
\section{Introduction}

\begin{center}
\begin{minipage}{12cm}
\emph{``The first principle is that you must not fool yourself and you are
the easiest person to fool. So you have to be very careful about that.
After you've not fooled yourself, it's easy not to fool other scientists.''}
\end{minipage}
\end{center}
\begin{flushright}
\citet[page 12] {Feynman1974}
\end{flushright}

% exploratory vs confirmatory also for simulation studies?

Statisticians frequently emphasize methodological flaws 
in research from other scientific disciplines. To name a few: 
Confidence intervals and $p$-values are misinterpreted, multiplicity is
not corrected, 
% the wrong inferential paradigm is chosen, 
$p$-hacking and HARKing inflate the Type-I error rate, selection bias, unblinded 
assessment, or observational data lead to confounded estimates of associations. 
These deficiencies are considered to be major drivers for
low reproducibility and replicability in many areas of science. 
It is therefore fairly surprising that the standards of empirical research in
statistics, especially the comparison of statistical methods with 
simulation studies or real data-sets, 
% simulation studies and benchmarking with real data sets, 
fall far short of the level of rigor that statisticians 
expect from their applied colleagues \citep{Boulesteix2020}.

Simulation studies are to a statistician what experiments are to a scientist.
They have become an ubiquitous tool for the evaluation of 
statistical methods, mainly because simulation can be used to
solve problems which cannot be solved using purely theoretical arguments.
% There is no single statistical method to rule them all and thus 
In this paper we mainly focus on simulation studies where the objective is
to compare the performance of two or more statistical methods 
(\emph{comparative simulation studies}).
Such studies are needed to ensure that previously proposed methods
work as expected under various conditions, and also to find conditions where
they fail. Moreover, evidence from
comparative simulation studies is often the only guidance for
statistics practitioners to choose from the plethora of available methods 
\citep{Boulesteix2013, Boulesteix2017b}.
% Moreover, advances in statistical software have made conducting a simulation
% study much easier than it used to be. 
% For example, it is
% straightforward to generate random numbers from common distributions with 
% the \textsf{R} programming language \citep{R2020} and there exist
% specialised \textsf{R}-packages, such as \texttt{SimDesign} \citep{Chalmers2020}, 
% that further simplify the process. 
% One may even argue that setting up a 
% simulation study has become so easy that a number of theoretical
% solutions might have been overlooked.

Just as real experiments, simulation studies require many decisions
to be made, for instance:
How will the data be generated? 
How many times will the simulation be repeated?
Which statistical methods will be compared and how are their parameters specified?
How will the performance of the methods be evaluated? 
The degree of flexibility, however, is much higher for simulation studies 
than for the real experiments, since they can be repeated under 
different conditions in short time at practically no cost.
This is why numerous guidelines and best practices for design, execution, and 
reporting of simulation studies have been proposed 
\citep{Hoaglin1975, Holford2000, Burton2006, Smith2010, OKelly2016, Monks2018, Elofsson2019, Morris2019, Boulesteix2020B},
we recommend \citet{Morris2019} for an introduction to state-of-the-art 
simulation study methodology. %for evaluating statistical methods. 

Despite the availability of these guidelines, statistics articles usually 
provide few details about the contained simulation studies. 
Journal policies sometimes require the computer code to reproduce the results,
but they rarely require or promote sound simulation methodology (\eg  
the preparation of a simulation protocol). This leaves %simulation study
researchers with considerable flexibility in how they conduct and present
simulations studies. 
% which may lead to over-optimistic conclusions. 
As a result, the reader of a statistics paper can rarely be sure about the 
quality of evidence that a simulation study provides. 
% This may ultimately lead
% to publication bias \citep{Boulesteix2015}

Statistical research is exploratory by nature, and of course not every 
simulation should require a simulation protocol or other stringent measures.
However, if the goal is to generate high-quality evidence, researchers
should adopt rigorous simulation methodology. 
The purpose of this paper is not to propose new guidelines, as they are all 
already available, but rather to highlight the need for
their adoption by the general statistics community. 
To this end, we list questionable research practices
(QRPs) related to comparative simulation studies (Section \ref{sec:QRP}) 
which may undermine their validity but can easily go undetected by
current standards. We then
show with an empirical study how easy it is to present any statistical method as an 
improvement over others, if QRPs remain undisclosed (Section \ref{sec:study}).
We also use the same example to show how over-optimism can be avoided if a sound 
methodological route is taken instead.
Similar studies have been conducted by \citet{Jelizarow2010} in the context
of data-set fishing and by \citet{Simmons2011} in the context of $p$-hacking
in psychological research, and were the main inspiration for this work. 
Finally, we provide simple and concrete suggestions for researchers, reviewers,
and other academic stakeholders to improve the methodological quality of 
comparative simulation studies (Section \ref{sec:discussion}).

% showed
% how a statistical method can be presented as superior if one looks long enough
% for a suitable data-set combined with varying characteristics of the method
% and pre-processing. \citet{Simmons2011} 

% Other relevant literature which I have found so far:
% \citet{Kerr1998, Boulesteix2013, Boulesteix2017,  Boulesteix2020}


\section{Questionable research practices} \label{sec:QRP}
\begin{itemize}
  \item Discuss all the QRPs, their relationship and their 
  consequences from Table \ref{table:QRPs}
\end{itemize}

\begin{table}[!htbp]
  \caption{Types of Questionable research practices (QRPs) in comparative simulation
  studies at different stages of the research process 
  (inspired by Table 1 from \citet{Wicherts2016}).}
  \label{table:QRPs}
  \centering
	\begin{longtable}[b]{p{.05\textwidth} p{.1\textwidth} p{.75\textwidth}}
		\toprule
		\textbf{Code} & \textbf{Related} & \textbf{Type of QRP} \\
  		\midrule
  		\multicolumn{2}{p{.15\textwidth}}{\textit{Design}} & \\ 
  		% \cmidrule{1-1}
  		D1 & R2 & Not/vaguely defining objectives of simulation study \\
  		D2 & E1 & Not/vaguely defining data-generating mechanism \\ 
  		%(data-generating mechanism, parameters of the method, etc.) \\
  		D3 & E2, E3 & Not/vaguely defining which methods will be compared and how their
  		parameters are specified \\ 
  		D4 & E4 & Not/vaguely defining evaluation criteria \\
  		D5 & E5 & Not computing required number of simulations to achieve 
  		reasonable precision \\
  		D6 & E6 & Not/vaguely defining how to handle ambiguous analytic situations 
  		(\eg non-convergence of methods) \\
  		\vdots & \vdots & \vdots \\[2em]
  
  		\multicolumn{2}{p{.15\textwidth}}{\textit{Execution}} & \\
  		% \cmidrule{1-1}
  		E1 & D2 & Adapting data-generating mechanism to achieve certain outcomes \\
  		E2 & D3 & Including/excluding certain methods based on their performance \\
  		E3 & D3 & Changing parameters of methods based on their performance \\
  		E4 & D4 & Choosing evaluation criteria based on outcome of simulations \\
  		E5 & D5 & Choosing number of simulations to achieve desired outcome \\
  		E6 & D6 & Including/excluding certain simulations to achieve desired outcome \\
  		\vdots & \vdots & \vdots \\[2em]
  		
  		
  		\multicolumn{2}{p{.15\textwidth}}{\textit{Reporting}} & \\
  		% \cmidrule{1-1}
  		R1 & & Selective reporting of results from simulation conditions
  		that lead to certain outcomes \\
  		R2 & D1 & Presenting exploratory simulation studies as confirmatory
  		(HARKing) \\
  		R3 & & Failing to report Monte Carlo uncertainty \\
  		R4 & & Failing to assure reproducibility (\eg sharing computer code and
  		details about computing environment) \\
  		R5 & & Failing to assure replicability (\eg reporting of design and 
  		execution	methodology) \\
  		\vdots & \vdots & \vdots \\
  		
		\bottomrule	
	\end{longtable}
\end{table}

\section{Empirical study: The Adaptive Importance Elastic Net (AINET)} \label{sec:study}

\todo{write intro and theory of ainet}
% The method should be:
% \begin{itemize}
%   \item suitable method for comparative simulation study, \eg method to estimate
%   one-dimensional parameter with CI?
%   \item several other methods available to which we can compare method
%   (and possibly remove superior methods from comparison)
%   \item no closed form solution? Or maybe closed form solutions so that we know
%   the true (bad) characteristics of the method?
%   \item ``promising'' and intuitive idea
%   \item tunable parameters such that we can fish for superiority of the method
%   \item several possible evaluation criteria so that we can pick the best one
%   \item easy to present graphically
%   (e.g. two subfigures: one the QRP approach, one the sound approach)
% \end{itemize}

\section{Discussion} \label{sec:discussion}

\begin{itemize}
  \item emphasize that researchers usually engage in QRPs without malicious 
  intent but because: ``people are self-serving in their interpretation
  of ambiguous information and remarkably adept at reaching justifiable 
  conclusions that mesh with their desires'' \citep{Simmons2011} and also 
  because ``superior'' methods are easier published (publication bias in 
  methods research \citep{Boulesteix2015}?)
  
  \item different stages of methodological research warrant different
  levels of stringency $\rightarrow$ draw comparison to clinical research
  
  \item separate exploratory from confirmatory research also in methodology
  
  \item Some suggestions for researchers: 
    \begin{itemize}
      \item use simulation protocols, follow the ``ADEMP'' approach from
      \citet{Morris2019}
      \item collaboration between different research groups (with possibly 
      ``competing'' methods)
      \item blind yourself when analyzing simulation results (\eg shuffle the
      method labels as proposed by \citet{Dutilh2019})
    \end{itemize}
  
  \item Some suggestions for reviewers, editors: 
    \begin{itemize}
      \item need to shift focus from ``showing superiority of method'' to
  ``showing where method works and where it fails''
    \item two ways to increase use of simulation protocols :
    \begin{itemize}
      \item[+] positive reinforcement: provide incentives for researchers, 
      \eg badges on papers that indicate that simulation has a protocol
      (very successful in psychology \citep{Kidwell2016})
      \item[-] negative reinforcement: require simulation protocol for 
      publication of comparison study (\eg similar to Cochrane systematic 
      reviews)
    \end{itemize}
    \item provide enough space for description of methodology, require 
    supplementary materials, etc.
    \end{itemize}
  
  
    
  \item more software needs to be developed that facilitates sound design, 
  execution, and reporting of simulation studies 
  \citep{White2010, Gasparini2018, Chalmers2020}
  
  
\end{itemize}

% Bibliography
% ======================================================================
\newpage
\bibliographystyle{apalikedoiurl}
\bibliography{bibliography}


% Appendix
% ======================================================================
% \begin{appendices}
% 
% \section{Appendix title}
% \label{appendix:xxxx}
% 
% \end{appendices}

\end{document}
