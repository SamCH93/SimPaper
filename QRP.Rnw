%% Template for a scientific paper by Samuel Pawel
%% Last modification: 17. December 2020
\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphics}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{textcomp}
\usepackage[sc]{mathpazo}
\usepackage{times}
\usepackage{doi} % automatic doi-links
\usepackage[round]{natbib} % bibliography
\usepackage{multirow} % multi rows and columns for tables
\usepackage{longtable} % tables that span several pages
\usepackage{booktabs} % nicer tables
\usepackage[toc, page]{appendix} % better appendices
\usepackage{nameref} % reference appendices with names
\usepackage{todonotes}

%% margins
%% ----------------------------------------------------------------------------
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 right=25mm,
 top=20mm,
 bottom=20mm,
 }

%% title, authors, affiliations, mail
%% ----------------------------------------------------------------------------
\newcommand\longtitle{Pitfalls and Potentials in Simulation Studies}
\newcommand\shorttitle{\longtitle} % if longtitle too long, change here
% \newcommand\subtitle{How undisclosed flexibility in simulation experiments
% allows to show superiority of any method}
\newcommand\subtitle{Questionable research practices in %Undisclosed flexibility in design, execution, and reporting of
comparative simulation studies allow to show superiority of any method}
\newcommand\longauthors{Samuel Pawel, Lucas Kook, Kelly Reeve}
\newcommand\shortauthors{S. Pawel, L. Kook, K. Reeve} % if longauthors too long, change here
\newcommand\affiliation{
  Epidemiology, Biostatistics and Prevention Institute  \\
  % Center for Reproducible Science (CRS) \\
  University of Zurich
}
\title{
  \vspace{-2em}
  \textbf{\longtitle} \\
  \subtitle
}
\author{
  \textbf{\longauthors} \\
  \affiliation \\
  \{samuel.pawel, lucasheinrich.kook, kelly.reeve\}@uzh.ch
}
\date{\today} % don't forget to hard-code date when submitting to arXiv!

%% hyperref options
%% ----------------------------------------------------------------------------
\usepackage{hyperref}  
\hypersetup{
  bookmarksopen=true, 
  breaklinks=true,
  pdftitle={\shorttitle}, 
  pdfauthor={\shortauthors},
  pdfsubject={},
  pdfkeywords={},
  colorlinks=true,
  linkcolor=RoyalPurple,
  anchorcolor=black,
  citecolor=MidnightBlue,
  urlcolor=BrickRed,
}

%% Headers and footers
%% ----------------------------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\shorttitle}
\rhead{\shortauthors}

%% Useful commands
%% ----------------------------------------------------------------------------
%% Shortforms
\newcommand{\eg}{\textit{e.\,g.\,}} % e.g.
\newcommand{\ie}{\textit{i.\,e.\,}} % i.e.
\newcommand{\cf}{\textit{cf\,}} % cf
%% Distributions
\DeclareMathOperator{\Ber}{B} % Bernoulli
\DeclareMathOperator{\Bin}{Bin} % Binomial Distribution
\DeclareMathOperator{\Cauchy}{C} % Cauchy Distribution
\DeclareMathOperator{\Po}{Po} % Poisson
\DeclareMathOperator{\Exp}{Exp} % Exponential
\DeclareMathOperator{\Nor}{N} % Normal
\DeclareMathOperator{\stud}{t} % Student
\DeclareMathOperator{\Ga}{G} % Gamma
\DeclareMathOperator{\Be}{Be} % Beta
%% Operators and special functions
\DeclareMathOperator{\Var}{Var} % Variance
\DeclareMathOperator{\E}{\mathsf{E}} % Expectation
\DeclareMathOperator{\Cov}{Cov} % Covariance
\DeclareMathOperator{\Corr}{Corr} % Correlation 
\DeclareMathOperator{\se}{se} % Standard error
\DeclareMathOperator{\sign}{sign} % Sign
\DeclareMathOperator{\logit}{logit} % Logit
\DeclareMathOperator{\Mod}{Mod} % Modus
\DeclareMathOperator{\Med}{Med} % Median
\DeclareMathOperator{\diag}{diag} % Diagonalmatrix
\DeclareMathOperator{\trace}{tr} % Trace
\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Probability
\newcommand{\p}{f} % Density function
\newcommand{\B}{\operatorname{{B}}} % Beta function
\newcommand{\Lik}{L} % Likelihood function
\DeclareMathOperator{\arctanh}{arctanh} % Arcus tangens hyperbolicus
\DeclareMathOperator*{\argmax}{arg\,max} % Argmax
\DeclareMathOperator*{\argmin}{arg\,min} % Argmin
%% Other math things
\newcommand{\C}{\mathbb{C}} % Complex
\newcommand{\R}{\mathbb{R}} % Reals
\newcommand{\Q}{\mathbb{Q}} % Rationals
\newcommand{\Z}{\mathbb{Z}} % Integers
\newcommand{\N}{\mathbb{N}} % Naturals
\newcommand{\given}{\,\vert\,} % Given
\newcommand{\abs}[1]{\left\lvert#1\right\rvert} % Absolutvalue
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % Norm
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil} % Ceiling
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor} % Floor
\newcommand{\sprod}[1]{\left\langle#1\right\rangle} % Scalarproduct
\newcommand{\Ind}[2]{\mathsf{I}_{#2}(#1)} % Indicatorfunction
\newcommand{\IdMat}{\boldsymbol{\mathrm{I}}} % Identity matrix
%% ----------------------------------------------------------------------------

\begin{document}
\maketitle

% knitr options
% ======================================================================
<< "main-setup", include = FALSE >>=
library(knitr)
opts_chunk$set(fig.height = 4,
               echo = FALSE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE, 
               eval = TRUE)
@

% Abstract
% ======================================================================
\begin{center}
\begin{minipage}{13cm}
{\small
\rule{\textwidth}{0.5pt} \\
{\centering \textbf{Abstract} \\
% Statisticians and methodologists often emphasize methodological flaws 
% in research from other scientific disciplines, yet the methodological 
% standards of empirical research in their own field are themeselves poor: 
Comparative simulation studies are a workhorse tool for benchmarking
% operating characteristics 
of statistical methods. But if done in nontransparent and uncritical manner, they
may lead to over-optimism. Furthermore, the current publication requirements of 
statistics journals cannot prevent questionable research practices
such as selective reporting of results which favour a particular method.
The past years have witnessed numerous suggestions and initiatives to 
improve on these issues, most notably the need for simulation protocols, 
but only little progress can be seen to date. 
% In this paper we illustrate with an empirical study how easy it is to fool
% others (and ourselves) using poor simulation study methodology. 
In this paper we list common questionable research practices which undermine
the validity of findings from comparative simulation studies. We illustrate with 
an empirical study how easy it is to fool
others (and ourselves) using poor simulation study methodology.
We also use the same example to show how over-optimism can be avoided if a sound 
methodological approach is adopted instead.
Finally, we provide simple and concrete suggestions for researchers, reviewers,
and other academic stakeholders to improve the methodological quality of 
comparative simulation studies.
% with minimal impact on the publication 
% process.
}
\rule{\textwidth}{0.4pt} \\
\textit{Key words}: 
Simulation studies, transparency, over-optimism, questionable research practices
}
\end{minipage}
\end{center}

% Jelizarow, M., Guillemot, V., Tenenhaus, A., Strimmer, K. and Boulesteix, A. L. (2010) Over-optimism in bioinformatics: An illustration. Bioinformatics, 
% --> about data sets, this article about simulation

% Introduction
% ======================================================================
\section{Introduction}

\begin{center}
\begin{minipage}{12cm}
\emph{``The first principle is that you must not fool yourself and you are
the easiest person to fool. So you have to be very careful about that.
After you've not fooled yourself, it's easy not to fool other scientists.''}
\end{minipage}
\end{center}
\begin{flushright}
\citet[page 12] {Feynman1974}
\end{flushright}

% exploratory vs confirmatory also for simulation studies?

% Statisticians frequently emphasize methodological flaws
% in research from other scientific disciplines. To name a few:
% Confidence intervals and $p$-values are misinterpreted, multiplicity is
% not corrected,
% % the wrong inferential paradigm is chosen,
% $p$-hacking and HARKing inflate the Type-I error rate, selection bias, unblinded
% assessment, or observational data lead to confounded estimates of associations.
% These deficiencies are considered to be major drivers for
% low reproducibility and replicability in many areas of science.
% It is therefore fairly surprising that the standards of empirical research in
% statistics, especially the comparison of statistical methods with
% simulation studies or real data-sets,
% % simulation studies and benchmarking with real data sets,
% fall far short of the level of rigor that statisticians
% expect from their applied colleagues \citep{Boulesteix2020}.

Simulation studies are to a statistician what experiments are to a scientist.
They have become an ubiquitous tool for the evaluation of 
statistical methods, mainly because simulation can be used to
solve problems which cannot be solved using purely theoretical arguments.
% There is no single statistical method to rule them all and thus 
In this paper we focus on simulation studies where the objective is to compare
the performance of two or more statistical methods (\emph{comparative simulation
  studies}). Such studies are needed to ensure that previously proposed methods
work as expected under various conditions, and also to find conditions where
they fail. Moreover, evidence from comparative simulation studies is often the
only guidance for statistics practitioners to choose from the plethora of
available methods \citep{Boulesteix2013, Boulesteix2017b}.
% Moreover, advances in statistical software have made conducting a simulation
% study much easier than it used to be. 
% For example, it is
% straightforward to generate random numbers from common distributions with 
% the \textsf{R} programming language \citep{R2020} and there exist
% specialised \textsf{R}-packages, such as \texttt{SimDesign} \citep{Chalmers2020}, 
% that further simplify the process. 
% One may even argue that setting up a 
% simulation study has become so easy that a number of theoretical
% solutions might have been overlooked.

Just as real experiments, comparative simulation studies require many decisions
to be made, for instance: How will the data be generated? How many times will
the simulation be repeated? Which statistical methods will be compared and how
are their parameters specified? How will the performance of the methods be
evaluated? The degree of flexibility, however, is much higher for simulation
studies than for the real experiments, since they can be repeated under
different conditions in short time at practically no cost. This is why numerous
guidelines and best practices for design, execution, and reporting of simulation
studies have been proposed \citep{Hoaglin1975, Holford2000, Burton2006,
  Smith2010, OKelly2016, Monks2018, Elofsson2019, Morris2019, Boulesteix2020B},
we recommend \citet{Morris2019} for an introduction to state-of-the-art
simulation study methodology. %for evaluating statistical methods.

Despite the availability of these guidelines, statistics articles usually 
provide few details about the contained simulation studies.
Journal policies sometimes require the computer code to reproduce the results,
but they rarely require or promote sound simulation methodology (\eg
the preparation of a simulation protocol). This leaves %simulation study
researchers with considerable flexibility in how they conduct and present
simulations studies. 
% which may lead to over-optimistic conclusions. 
As a result, the reader of a statistics paper can rarely be sure about the 
quality of evidence that a simulation study provides. 
% This may ultimately lead
% to publication bias \citep{Boulesteix2015}

Statistical research is both, exploratory and confirmatory, so not every
simulation study requires a simulation protocol or other stringent measures.
However, if the goal is to generate high-quality evidence, researchers should
adopt rigorous simulation methodology. The aim of this paper is % not to
% propose new guidelines, as they are all already available, but rather
% but to highlight the need for their adoption by the general statistics
% community.
to highlight the need for the adoption of more rigorous simulation methodolgoy
by the general statistics community. To this end, we list questionable research
practices (QRPs) related to comparative simulation studies (Section
\ref{sec:QRP}) which may undermine their validity but can easily go undetected
by current standards. Researchers do usually not engage in these QRPs with ill
intent, but because of their subconscious biases, hopes, and expectations. We
then show with an empirical study how easy it is to present any statistical
method as an improvement over others, if QRPs remain undisclosed (Section
\ref{sec:study}). We also use the same example to show how over-optimism can be
avoided if a sound methodological route is taken instead. Similar studies have
been conducted by \citet{Niessl2021} and \citet{Jelizarow2010}, and by
\citet{Simmons2011} in the context of $p$-hacking in psychological research, and
were the main inspiration for this work. Finally, we provide simple and concrete
suggestions for researchers, reviewers, and other academic stakeholders to
improve the methodological quality of comparative simulation studies (Section
\ref{sec:discussion}).

% showed
% how a statistical method can be presented as superior if one looks long enough
% for a suitable data-set combined with varying characteristics of the method
% and pre-processing. \citet{Simmons2011} 

% Other relevant literature which I have found so far:
% \citet{Kerr1998, Boulesteix2013, Boulesteix2017,  Boulesteix2020}


\section{Questionable research practices in comparative simulation
  studies} \label{sec:QRP}
Table~\ref{table:QRPs} lists different types of QRPs which threathen the
validity of comparative simulation studies. They are categorized with respect to
the stage of research at which they can occur.

\begin{table}[!htb]
  \caption{Types of Questionable research practices (QRPs) in comparative simulation
  studies at different stages of the research process 
  (inspired by Table 1 from \citet{Wicherts2016}).}
  \label{table:QRPs}
  \centering
	\begin{longtable}[b]{p{.05\textwidth} p{.1\textwidth} p{.75\textwidth}}
		\toprule
		\textbf{Code} & \textbf{Related} & \textbf{Type of QRP} \\
  		\midrule
    \multicolumn{2}{p{.15\textwidth}}{\textit{Design}} & \\
  		% \cmidrule{1-1}
    D1 & R2 & Not/vaguely defining objectives of simulation study \\
    D2 & E1 & Not/vaguely defining data-generating mechanism \\
  		%(data-generating mechanism, parameters of the method, etc.) \\
    D3 & E2, E3 & Not/vaguely defining which methods will be compared and how their
                  parameters are specified \\
    D4 & E4 & Not/vaguely defining evaluation criteria \\
    D5 & E6 & Not/vaguely defining how to handle ambiguous analytic situations
              (\eg non-convergence of methods) \\
    D6 & E5 & Not computing required number of simulations to achieve
              desired precision \\
    \vdots & \vdots & \vdots \\[2em]
  
    \multicolumn{2}{p{.15\textwidth}}{\textit{Execution}} & \\
  		% \cmidrule{1-1}
    E1 & D2 & Adapting data-generating mechanism to achieve certain outcomes \\
    E2 & D3 & Including/excluding certain methods based on their performance \\
    E3 & D3 & Changing parameters of methods based on their performance \\
    E4 & D4 & Choosing evaluation criteria based on outcome of simulations \\
    E5 & D5 & Choosing number of simulations to achieve desired outcome \\
    E6 & D6 & Including/excluding certain simulations to achieve desired outcome \\
    \vdots & \vdots & \vdots \\[2em]
  		
  		
    \multicolumn{2}{p{.15\textwidth}}{\textit{Reporting}} & \\
  		% \cmidrule{1-1}
    R1 & & Selective reporting of results from simulation conditions
           that lead to certain outcomes \\
    R2 & D1 & Presenting exploratory simulation studies as confirmatory
              (HARKing) \\
    R3 & & Failing to report Monte Carlo uncertainty \\
    R4 & & Failing to assure reproducibility (\eg sharing computer code and
           details about computing environment) \\
    R5 & & Failing to assure replicability (\eg reporting of design and
           execution	methodology) \\
    \vdots & \vdots & \vdots \\
  		
		\bottomrule	
	\end{longtable}
\end{table}

\subsection{Design}
The \emph{a priori} specification of research hypotheses, study design, and
analytic choices is what separates \emph{confirmatory} from \emph{exploratory}
research. To prevent over-optimistic study conclusions and allow research
consumers to distinguish between confirmatory and exploratory research, many
non-methodological journals require preregistration of study design and analysis
protocols. % This
% practice has not been adopted by the statistics community.
If researchers attempt to conduct confirmatory research, it is generally
recommended to write a simulation protocol \citep{Morris2019}. In contrast,
vaguely or not defining the goals and means of a simulation study can be
considered a QRP. This is arguably even more important than in
non-methodological research because the multiplicity involved in simulation
studies is far higher \citep{Hoffmann2021}. For instance, not a priori defining
the data-generating mechanism (D2), the methods under investigation (D3), or the
evaluation metrics (D4) leaves a high number of \emph{researcher degrees of
  freedom} open, which increase the chance of over-optimistic conclusions.

Another crucial part of rigorous design is a sample size calculation, see
Section 5.3 in \citet{Morris2019} for an overview. By failing to conduct a
sample size calculation (D5), researchers are a at a higher risk of drawing the
wrong conclusions (if their sample size is too small), or wasting computer
resources (if their sample size is too large).

\dots

\subsection{Execution}
If researchers have prespecified their simulation study, its execution is mostly
just following their simulation protocol. However, if no protocol is in place,
they can exploit various QRPs in order to increase the chance for showing
superiority of a particular method.

For example, if their favored method does not perform better than its
competitors under the data-generating mechanism they initially had in mind, they
can adapt the mechanism (E1) until they find conditions where their method is
superior.

If researchers are unable to find any data-generating mechanisms under which
their favored method beats a competitor method, they can exclude the competitor
from the comparison (E2). Similarly, to make their favored method look better,
additional methods may be included in the simulation study which perform worse
under the (adapted) data-generating mechanisms.

The methods under comparison may come with hyper-/tuning-parameters (for example
\dots). Researchers can tune the parameters of their favored method to achieve
superiority over others (E3), while leaving the parameters of the competitor
methods at their default values, or even search for parameters which make them
look worse.

The evaluation criteria for comparing the performance of the investigated
methods can be changed to make a particular method look better than the others
(E4). For example, \dots Similarly, the whole objective of the simulation study
may be changed, for example, an initial comparison of predictive performance of
methods may be changed to comparing estimation performance of methods when the
researchers realizes that their favored method performs worse in prediction
tasks.

If no sample size calculation has been conducted a priori, researchers can
choose the number of simulations until they arrive at their desired conclusion
(E5). If the number of simulations is small (relative to the noise level), the
risk for a false positive findings are drastically higher. Similarly,
researchers can ``tune'' the random numbers by changing the initialization seed
of their random number generator until they find a realization where their
preferred method seems superior.

It can happen that methods fail to converge an therefore fail to output a result
in a particular simulation. If researchers do not prespecify how they will
handle these situations, they can try different inclusion/exclusion or
imputations strategies until their favored method looks superior.


\subsection{Reporting}



\section{Empirical study: The Adaptive Importance Elastic Net (AINET)} \label{sec:study}

\todo{write intro and theory of ainet}
% The method should be:
% \begin{itemize}
%   \item suitable method for comparative simulation study, \eg method to estimate
%   one-dimensional parameter with CI?
%   \item several other methods available to which we can compare method
%   (and possibly remove superior methods from comparison)
%   \item no closed form solution? Or maybe closed form solutions so that we know
%   the true (bad) characteristics of the method?
%   \item ``promising'' and intuitive idea
%   \item tunable parameters such that we can fish for superiority of the method
%   \item several possible evaluation criteria so that we can pick the best one
%   \item easy to present graphically
%   (e.g. two subfigures: one the QRP approach, one the sound approach)
% \end{itemize}

\section{Discussion} \label{sec:discussion}

\begin{table}[!htb]
  \caption{Recommendations to improve rigour of comparative simulation studies
    and prevent QRPs}
  \label{table:recommendations}
  \centering
	\begin{longtable}[b]{p{.95\textwidth}}
		\toprule
    \textit{Researchers} \\
    \midrule
    \dots \\

    \textit{Journals editors and reviewers} \\
    \midrule
    \dots \\


    \textit{Funders} \\
    \midrule
    \dots \\


		\bottomrule
	\end{longtable}
\end{table}

\begin{itemize}
  \item emphasize that researchers usually engage in QRPs without malicious 
  intent but because: ``people are self-serving in their interpretation
  of ambiguous information and remarkably adept at reaching justifiable 
  conclusions that mesh with their desires'' \citep{Simmons2011} and also 
  because ``superior'' methods are easier published (publication bias in 
  methods research \citep{Boulesteix2015}?)
  
  \item different stages of methodological research warrant different
  levels of stringency $\rightarrow$ draw comparison to clinical research
  
  \item separate exploratory from confirmatory research also in methodology
  
  \item Some suggestions for researchers: 
    \begin{itemize}
      \item use simulation protocols, follow the ``ADEMP'' approach from
      \citet{Morris2019}
      \item collaboration between different research groups (with possibly 
      ``competing'' methods)
      \item blind yourself when analyzing simulation results (\eg shuffle the
      method labels as proposed by \citet{Dutilh2019})
    \end{itemize}
  
  \item Some suggestions for reviewers, editors: 
    \begin{itemize}
      \item need to shift focus from ``showing superiority of method'' to
  ``showing where method works and where it fails''
    \item two ways to increase use of simulation protocols :
    \begin{itemize}
      \item[+] positive reinforcement: provide incentives for researchers, 
      \eg badges on papers that indicate that simulation has a protocol
      (very successful in psychology \citep{Kidwell2016})
      \item[-] negative reinforcement: require simulation protocol for 
      publication of comparison study (\eg similar to Cochrane systematic 
      reviews)
    \end{itemize}
    \item provide enough space for description of methodology, require 
    supplementary materials, etc.
    \end{itemize}
  
  
    
  \item more software needs to be developed that facilitates sound design, 
  execution, and reporting of simulation studies 
  \citep{White2010, Gasparini2018, Chalmers2020}
  
  
\end{itemize}

% Bibliography
% ======================================================================
\newpage
\bibliographystyle{apalikedoiurl}
\bibliography{bibliography}


% Appendix
% ======================================================================
% \begin{appendices}
% 
% \section{Appendix title}
% \label{appendix:xxxx}
% 
% \end{appendices}

\end{document}
