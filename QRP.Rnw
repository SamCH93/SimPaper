%% Template for a scientific paper by Samuel Pawel
%% Last modification: 17. December 2020
\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphics}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{textcomp}
\usepackage[sc]{mathpazo}
\usepackage{times}
\usepackage{doi} % automatic doi-links
\usepackage[round]{natbib} % bibliography
\usepackage{multirow} % multi rows and columns for tables
\usepackage{longtable} % tables that span several pages
\usepackage{booktabs} % nicer tables
\usepackage[toc, page]{appendix} % better appendices
\usepackage{nameref} % reference appendices with names
\usepackage{todonotes}

%% margins
%% ----------------------------------------------------------------------------
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 right=25mm,
 top=20mm,
 bottom=20mm,
 }

%% title, authors, affiliations, mail
%% ----------------------------------------------------------------------------
\newcommand\longtitle{Pitfalls and Potentials in Simulation Studies}
\newcommand\shorttitle{\longtitle} % if longtitle too long, change here
% \newcommand\subtitle{How undisclosed flexibility in simulation experiments
% allows to show superiority of any method}
\newcommand\subtitle{Questionable research practices in %Undisclosed flexibility in design, execution, and reporting of
comparative simulation studies allow to show superiority of any method}
\newcommand\longauthors{Samuel Pawel, Lucas Kook, Kelly Reeve}
\newcommand\shortauthors{S. Pawel, L. Kook, K. Reeve} % if longauthors too long, change here
\newcommand\affiliation{
  Epidemiology, Biostatistics and Prevention Institute  \\
  % Center for Reproducible Science (CRS) \\
  University of Zurich
}
\title{
  \vspace{-2em}
  \textbf{\longtitle} \\
  \subtitle
}
\author{
  \textbf{\longauthors} \\
  \affiliation \\
  \{samuel.pawel, lucasheinrich.kook, kelly.reeve\}@uzh.ch
}
\date{\today} % don't forget to hard-code date when submitting to arXiv!

%% hyperref options
%% ----------------------------------------------------------------------------
\usepackage{hyperref}  
\hypersetup{
  bookmarksopen=true, 
  breaklinks=true,
  pdftitle={\shorttitle}, 
  pdfauthor={\shortauthors},
  pdfsubject={},
  pdfkeywords={},
  colorlinks=true,
  linkcolor=RoyalPurple,
  anchorcolor=black,
  citecolor=MidnightBlue,
  urlcolor=BrickRed,
}

%% Headers and footers
%% ----------------------------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\shorttitle}
\rhead{\shortauthors}

%% Useful commands
%% ----------------------------------------------------------------------------
%% Shortforms
\newcommand{\eg}{\textit{e.\,g.\,}} % e.g.
\newcommand{\ie}{\textit{i.\,e.\,}} % i.e.
\newcommand{\cf}{\textit{cf\,}} % cf
\newcommand{\vs}{\textit{vs\,}} % cf
%% Distributions
\DeclareMathOperator{\Ber}{B} % Bernoulli
\DeclareMathOperator{\Bin}{Bin} % Binomial Distribution
\DeclareMathOperator{\Cauchy}{C} % Cauchy Distribution
\DeclareMathOperator{\Po}{Po} % Poisson
\DeclareMathOperator{\Exp}{Exp} % Exponential
\DeclareMathOperator{\Nor}{N} % Normal
\DeclareMathOperator{\stud}{t} % Student
\DeclareMathOperator{\Ga}{G} % Gamma
\DeclareMathOperator{\Be}{Be} % Beta
%% Operators and special functions
\DeclareMathOperator{\Var}{Var} % Variance
\DeclareMathOperator{\E}{\mathsf{E}} % Expectation
\DeclareMathOperator{\Cov}{Cov} % Covariance
\DeclareMathOperator{\Corr}{Corr} % Correlation 
\DeclareMathOperator{\se}{se} % Standard error
\DeclareMathOperator{\sign}{sign} % Sign
\DeclareMathOperator{\logit}{logit} % Logit
\DeclareMathOperator{\Mod}{Mod} % Modus
\DeclareMathOperator{\Med}{Med} % Median
\DeclareMathOperator{\diag}{diag} % Diagonalmatrix
\DeclareMathOperator{\trace}{tr} % Trace
\renewcommand{\P}{\operatorname{\mathsf{Pr}}} % Probability
\newcommand{\p}{f} % Density function
\newcommand{\B}{\operatorname{{B}}} % Beta function
\newcommand{\Lik}{L} % Likelihood function
\DeclareMathOperator{\arctanh}{arctanh} % Arcus tangens hyperbolicus
\DeclareMathOperator*{\argmax}{arg\,max} % Argmax
\DeclareMathOperator*{\argmin}{arg\,min} % Argmin
%% Other math things
\newcommand{\C}{\mathbb{C}} % Complex
\newcommand{\R}{\mathbb{R}} % Reals
\newcommand{\Q}{\mathbb{Q}} % Rationals
\newcommand{\Z}{\mathbb{Z}} % Integers
\newcommand{\N}{\mathbb{N}} % Naturals
\newcommand{\given}{\,\vert\,} % Given
\newcommand{\abs}[1]{\left\lvert#1\right\rvert} % Absolutvalue
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % Norm
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil} % Ceiling
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor} % Floor
\newcommand{\sprod}[1]{\left\langle#1\right\rangle} % Scalarproduct
\newcommand{\Ind}[2]{\mathsf{I}_{#2}(#1)} % Indicatorfunction
\newcommand{\IdMat}{\boldsymbol{\mathrm{I}}} % Identity matrix
%% ----------------------------------------------------------------------------

\newcommand{\ainet}{\textsc{ainet}}
\newcommand{\Lucas}[1]{\textcolor{blue!80}{Lucas: #1}}

\begin{document}
\maketitle

% knitr options
% ======================================================================
<< "main-setup", include = FALSE >>=
library(knitr)
opts_chunk$set(fig.height = 4,
               echo = FALSE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE, 
               eval = TRUE)
@

% Abstract
% ======================================================================
\begin{center}
\begin{minipage}{13cm}
{\small
\rule{\textwidth}{0.5pt} \\
{\centering \textbf{Abstract} \\
% Statisticians and methodologists often emphasize methodological flaws 
% in research from other scientific disciplines, yet the methodological 
% standards of empirical research in their own field are themeselves poor: 
Comparative simulation studies are a workhorse tool for benchmarking
% operating characteristics 
of statistical methods. But if done in nontransparent and uncritical manner, they
may lead to over-optimism. Furthermore, the current publication requirements of 
statistics journals cannot prevent questionable research practices
such as selective reporting of results which favour a particular method.
The past years have witnessed numerous suggestions and initiatives to 
improve on these issues, most notably the need for simulation protocols, 
but only little progress can be seen to date. 
% In this paper we illustrate with an empirical study how easy it is to fool
% others (and ourselves) using poor simulation study methodology. 
In this paper we list common questionable research practices which undermine
the validity of findings from comparative simulation studies. We illustrate with 
an empirical study how easy it is to fool
others (and ourselves) using poor simulation study methodology.
We also use the same example to show how over-optimism can be avoided if a sound 
methodological approach is adopted instead.
Finally, we provide simple and concrete suggestions for researchers, reviewers,
and other academic stakeholders to improve the methodological quality of 
comparative simulation studies.
% with minimal impact on the publication 
% process.
}
\rule{\textwidth}{0.4pt} \\
\textit{Key words}: 
Simulation studies, transparency, over-optimism, questionable research practices
}
\end{minipage}
\end{center}

% Jelizarow, M., Guillemot, V., Tenenhaus, A., Strimmer, K. and Boulesteix, A. L. (2010) Over-optimism in bioinformatics: An illustration. Bioinformatics, 
% --> about data sets, this article about simulation

% Introduction
% ======================================================================
\section{Introduction}

\begin{center}
\begin{minipage}{12cm}
\emph{``The first principle is that you must not fool yourself and you are
the easiest person to fool. So you have to be very careful about that.
After you've not fooled yourself, it's easy not to fool other scientists.''}
\end{minipage}
\end{center}
\begin{flushright}
\citet[page 12] {Feynman1974}
\end{flushright}

% exploratory vs confirmatory also for simulation studies?

% Statisticians frequently emphasize methodological flaws
% in research from other scientific disciplines. To name a few:
% Confidence intervals and $p$-values are misinterpreted, multiplicity is
% not corrected,
% % the wrong inferential paradigm is chosen,
% $p$-hacking and HARKing inflate the Type-I error rate, selection bias, unblinded
% assessment, or observational data lead to confounded estimates of associations.
% These deficiencies are considered to be major drivers for
% low reproducibility and replicability in many areas of science.
% It is therefore fairly surprising that the standards of empirical research in
% statistics, especially the comparison of statistical methods with
% simulation studies or real data-sets,
% % simulation studies and benchmarking with real data sets,
% fall far short of the level of rigor that statisticians
% expect from their applied colleagues \citep{Boulesteix2020}.

Simulation studies are to a statistician what experiments are to a scientist.
They have become an ubiquitous tool for the evaluation of 
statistical methods, mainly because simulation can be used to
solve problems which cannot be solved using purely theoretical arguments.
% There is no single statistical method to rule them all and thus 
In this paper we focus on simulation studies where the objective is to compare
the performance of two or more statistical methods (\emph{comparative simulation
  studies}). Such studies are needed to ensure that previously proposed methods
work as expected under various conditions, and also to find conditions where
they fail. Moreover, evidence from comparative simulation studies is often the
only guidance for statistics practitioners to choose from the plethora of
available methods \citep{Boulesteix2013, Boulesteix2017b}.
% Moreover, advances in statistical software have made conducting a simulation
% study much easier than it used to be. 
% For example, it is
% straightforward to generate random numbers from common distributions with 
% the \textsf{R} programming language \citep{R2020} and there exist
% specialised \textsf{R}-packages, such as \texttt{SimDesign} \citep{Chalmers2020}, 
% that further simplify the process. 
% One may even argue that setting up a 
% simulation study has become so easy that a number of theoretical
% solutions might have been overlooked.

Just as real experiments, comparative simulation studies require many decisions
to be made, for instance: How will the data be generated? How many times will
the simulation be repeated? Which statistical methods will be compared and how
are their parameters specified? How will the performance of the methods be
evaluated? The degree of flexibility, however, is much higher for simulation
studies than for the real experiments as they can be repeated under different
conditions in short time at practically no cost. This is why numerous guidelines
and best practices for design, execution, and reporting of simulation studies
have been proposed \citep{Hoaglin1975, Holford2000, Burton2006, Smith2010,
  OKelly2016, Monks2018, Elofsson2019, Morris2019, Boulesteix2020B}. We
recommend \citet{Morris2019} for an introduction to state-of-the-art simulation
study methodology. %for evaluating statistical methods.

Despite the availability of these guidelines, statistics articles usually 
provide few details about the contained simulation studies.
Journal policies sometimes require the computer code to reproduce the results,
but they rarely require or promote sound simulation methodology (\eg
the preparation of a simulation protocol). This leaves %simulation study
researchers with considerable flexibility in how they conduct and present
simulations studies. 
% which may lead to over-optimistic conclusions. 
As a result, the reader of a statistics paper can rarely be sure about the 
quality of evidence that a simulation study provides. 
% This may ultimately lead
% to publication bias \citep{Boulesteix2015}

Unfortunately, there are many questionable research practices (QRPs) which may
undermine the validity of comparative simulations studies, and which can easily
go undetected by current standards. Researchers do usually not engage QRPs with
ill intent, but because of their subconscious biases, hopes, and expectations.
Often there is a fine line between QRPs and legitimate research practices.
% However, if the goal of a simulation study is to generate high-quality evidence,
% researchers should adopt rigorous simulation methodology.

% Statistical research is both, exploratory and confirmatory, so not every
% simulation study requires a simulation protocol or other stringent measures.
The aim of this paper is % not to
% propose new guidelines, as they are all already available, but rather
% but to highlight the need for their adoption by the general statistics
% community.
to highlight the need for the adoption of higher standards in comparative
simulations study by the general statistics community. To this end, we list QRPs
related to comparative simulation studies (Section \ref{sec:QRP}). With an
actual simulation study, we then show how easy it is to a present a novel,
made-up method as an improvement over others if QRPs remain undisclosed (Section
\ref{sec:study}). We also use the same example to show how over-optimism can be
avoided if a sound methodological route is taken instead. Similar studies have
been conducted by \citet{Niessl2021} and \citet{Jelizarow2010}, and by
\citet{Simmons2011} in the context of $p$-hacking in psychological research, and
were the main inspiration for this work. Finally, we provide simple and concrete
suggestions for researchers, reviewers, and other academic stakeholders to
improve the methodological quality of comparative simulation studies (Section
\ref{sec:discussion}).

% showed
% how a statistical method can be presented as superior if one looks long enough
% for a suitable data-set combined with varying characteristics of the method
% and pre-processing. \citet{Simmons2011} 

% Other relevant literature which I have found so far:
% \citet{Kerr1998, Boulesteix2013, Boulesteix2017,  Boulesteix2020}


\section{Questionable research practices in comparative simulation
  studies} \label{sec:QRP} Table~\ref{table:QRPs} lists different types of QRPs
which threathen the validity of comparative simulation studies. They are
categorized with respect to the stage of research at which they can occur. The
classification is inspired by the classification of QRPs in experimental
research from \citet{Wicherts2016}.

\begin{table}[!htb]
  \caption{Types of Questionable research practices (QRPs) in comparative simulation
  studies at different stages of the research process.}
  \label{table:QRPs}
  \centering
	\begin{longtable}[b]{p{.05\textwidth} p{.1\textwidth} p{.75\textwidth}}
		\toprule
		\textbf{Code} & \textbf{Related} & \textbf{Type of QRP} \\
  		\midrule
    \multicolumn{2}{p{.15\textwidth}}{\textit{Design}} & \\
  		% \cmidrule{1-1}
    D1 & R2 & Not/vaguely defining objectives of simulation study \\
    D2 & E1 & Not/vaguely defining data-generating mechanism \\
  		%(data-generating mechanism, parameters of the method, etc.) \\
    D3 & E2, E3 & Not/vaguely defining which methods will be compared and how their
                  parameters are specified \\
    D4 & E4 & Not/vaguely defining evaluation criteria \\
    D5 & E6 & Not/vaguely defining how to handle ambiguous analytic situations
              (\eg non-convergence of methods) \\
    D6 & E5 & Not computing required number of simulations to achieve
              desired precision \\[1em]
  
    \multicolumn{2}{p{.15\textwidth}}{\textit{Execution}} & \\
  		% \cmidrule{1-1}
    E1 & D2 & Adapting data-generating mechanism to achieve certain outcomes \\
    E2 & D3 & Including/excluding certain methods based on their performance \\
    E3 & D3 & Changing parameters of methods based on their performance \\
    E4 & D4 & Choosing evaluation criteria based on outcome of simulations \\
    E5 & D5 & Choosing number of simulations to achieve desired outcome \\
    E6 & D6 & Including/excluding certain simulations to achieve desired outcome \\[1em]
  		
  		
    \multicolumn{2}{p{.15\textwidth}}{\textit{Reporting}} & \\
  		% \cmidrule{1-1}
    R1 & & Selective reporting of results from simulation conditions
           that lead to certain outcomes \\
    R2 & D1 & Presenting exploratory simulation studies as confirmatory
              (HARKing) \\
    R3 & & Failing to report Monte Carlo uncertainty \\
    R4 & & Failing to assure reproducibility (\eg sharing computer code and
           details about computing environment) \\
    R5 & & Failing to assure replicability (\eg reporting of design and
           execution	methodology) \\
		\bottomrule	
	\end{longtable}
\end{table}

\subsection{Design}
The \emph{a priori} specification of research hypotheses, study design, and
analytic choices is what separates \emph{confirmatory} from \emph{exploratory}
research. Often, large comparative simulation studies follow after a new method
has been proposed (either theoretically or with a smaller simulation study).
\Lucas{Often, large comparative simulation studies follow after a new method has
  been proposed (either theoretically or with a smaller simulation study). For
  instance, https://doi.org/10.1002/sim.9348. We could mention some as positive
  examples of confirmatory research, if they have a protocol.} To % prevent
% over-optimistic study conclusions and
allow research consumers to distinguish between confirmatory and exploratory
research, many non-methodological journals require preregistration of study
design and analysis protocols. % This
% practice has not been adopted by the statistics community.
If researchers attempt to conduct confirmatory research, it is generally
recommended to write a simulation protocol \citep{Morris2019}. In contrast,
vaguely or not defining the goals and means of a simulation study can be
considered a QRP. This is arguably even more important than in
non-methodological research because the multiplicity involved in simulation
studies is far higher \citep{Hoffmann2021}. For instance, failing to define a
priori the data-generating mechanism (D2), the methods under investigation (D3),
or the evaluation metrics (D4) leaves a high number of \emph{researcher degrees
  of freedom} open which increase the chance of over-optimistic conclusions.

Another crucial part of rigorous design is a sample size calculation, see
Section 5.3 in \citet{Morris2019} for an overview. A thorough planning of the
number of simulations in terms of expected precision of the primary estimand is
important. An arbitrarily chosen (but often too small) number of simulations
yields more heterogeneous results, but can be executed faster. By failing to
conduct a \Lucas{Rather ``simulation size''? Could be confused with the actual
  sample size} sample size calculation (D6), researchers are a at a higher risk
of drawing the wrong conclusions (if their sample size is too small), or wasting
computer resources (if their sample size is too large).


\subsection{Execution}
If researchers have prespecified their simulation study, they simply have to
follow their protocol in the execution phase. \Lucas{``simply'' is maybe a bit
strong. While planning the simulation study, it is virtually impossible to think
of all potential weaknesses or problems that may come up when actually
the study. However, conducting a preliminary simulation and code review can
obviate most of these problems. We should mention this somewhere.}
However, if no protocol is in place, they can engage in various QRPs in order to
increase the chance for showing superiority of their preferred method.

If under the data-generating mechanism the researcher initially had in mind,
their favored method does not perform better than its competitors, they can
adapt the mechanism until they find conditions where their method is superior
(E1). For example, they can change noise levels, the number of predictors, or
the effect sizes. It is usually not difficult to find reasonable justification
for such modifications.

If researchers are unable to find any data-generating mechanisms under which
their favored method beats a competing method, they can exclude the competitor
from the comparison (E2). To make their favored method look better, additional
methods which perform worse under the (adapted) data-generating mechanisms may
also be included in the simulation study.

The methods under comparison may come with hyper-/tuning-parameters (for example
\dots). Researchers can tune the parameters of their favored method to achieve
superiority over others (E3), while leaving the parameters of the competitor
methods at their default values, or even search for parameter values which make them
look worse.

The evaluation criteria for comparing the performance of the investigated
methods can be changed to make a particular method look better than the others
(E4). For example, a researcher who initially wanted to compare predictive
performance of methods using the Brier score, could change to area under the
curve if they realize that their method performs better with respect to the
latter measure. Similarly, the objective of the simulation study may also be
changed depending on the outcome, \eg an initial comparison of predictive
performance may be changed to comparing estimation performance when the
researcher realizes that their favored method performs worse in prediction
tasks.

If no a priori sample size calculation was conducted, researchers can choose the
number of simulations until they arrive at their desired conclusion (E5). If the
number of simulations is small (relative to the noise level), the risk that a
positive finding is a false positive increases. Similarly, when too few
simulations are conducted, researchers can ``tune'' the random numbers by
changing the initialization seed of their random number generator until they
find a realization where their preferred method seems superior.
\Lucas{Random number sharing to reduce Monte Carlo error in favor of a method is
discussed in \citet{Morris2019}.}

It can happen that methods fail to converge and thus not output a result in a
particular simulation. If researchers do not prespecify how they will handle
these situations, they can try different inclusion/exclusion or imputations
strategies until their favored method appears superior. Choosing an inadequate
strategy can result in systematic bias and misleading conclusions.

\subsection{Reporting}
In the reporting stage, researchers are faced with the challenge of reporting
the design, results, and analyses of their simulation study in a digestible
manner. Various QRPs can be employed to make a particular method look better.

Researchers may selectively report results from simulation conditions in which
their favored method performed best (R1). Failing to mention conditions in which
the method was inferior (or at least not superior) to competitors creates
over-optimistic impressions, and may lead readers to think that the method
uniformly outperforms competitors. Similarly, when researchers conduct
exploratory simulation studies but present their results as if they were
hypothesized \emph{a priori} (R2), this leads to over-confidence in the method.

Failing to report Monte Carlo uncertainty (R3), \eg error bars or confidence
intervals, is a QRP with various consequences \citep{van2019communicating}. It
confounds the research consumers ability to assess the quality of evidence from
the simulation study. It also allows the researcher to present random
differences in performance as if they were systematic.

Finally, by failing to assure reproducibility of the simulation study (R4),
coding errors with fatal consequences may go undetected. Moreover, by not
reporting the design and execution of the study in enough detail, other
researcher are unable to replicate and expand on the simulation study (R5).

\section{Empirical study: The Adaptive Importance Elastic Net
  (AINET)} \label{sec:study}

To illustrate the application of QRPs from Table~\ref{table:QRPs}, we conducted
a simulation study. The objective of the study was to evaluate the predictive
performance of a made-up regression method termed the ``adaptive importance
elastic net'' (\ainet). The main idea of \ainet{} is to use the variable
importance measure from a random forest for a weighted penalization of the
variables in an elastic net regression model. The hope is that this ad hoc
modification improves predictive performance in clinical prediction modeling
where penalized regression models are frequently used. Superficially, \ainet{}
seems sensible, however, for a linear data generating process no advantage over
the classical adaptive elastic net is expected. For more details on the method,
we refer to the simulation protocol \citep{pawel2022protocol}.


\subsection{Altering the data generating process (E1)}
The simulation study was planned with a low- to high-dimensional linear-logistic
data generating process. We could not detect a significant performance increase
of \ainet{} over standard logistic regression or elastic net regression in any
of the scenarios specified in the protocol. After a few modifications of the
data generating process, we finally found that \ainet{} outmatches logistic
regression for the combination of only few variables being associated with the
outcome (high sparsity), a non-linear effect, and a low number of events per
variable (EPV). 
% \todo{Make Figure: rho = 0.95 original -> rho 0.95, sparsity =
% 0.9, non-lin GLM improves}

%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--
\begin{figure}[!htb]
  \includegraphics[width = \textwidth]{code/E1.pdf}
  \caption{Shown are differences in Brier score betwen \ainet{} and random
    forest (RF), logistic regression (GLM), elastic net (EN), and adaptive
    elastic net (AEN). Arrows point from pre-registered simulation result to
    result from simulation where a specifically tweaked type of non-linear
    effect was added to the data generating process.} \label{fig:E1}
\end{figure}
%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--


\subsection{Removing comparators (E3)}
In the simulation protocol we specified standard logistic regression, random
forest, standard elastic net and adaptive elastic net as the appropriate
competitor methods for \ainet{}. However, we observed only minor (if any)
improvements of \ainet{} over the elastic net. In order to make \ainet{} look
better we could omit the comparisons with the elastic net by arguing that solely
the comparison among adaptive methods is relevant (adaptive elastic net \vs
\ainet).

%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--
\begin{figure}[!htb]
  \includegraphics[width = \textwidth]{code/E3.pdf}
  \caption{Differences in Brier score betwen \ainet{} and random forest (RF), 
    logistic regression (GLM), elastic net (EN), and adaptive elastic net (AEN).
    EN is shown more transparently to emphasize the results of the remaining 
    contrasts. A subset of the simulation results with $\rho = 0.95$ and
    $\operatorname{prev} = 0.05$ is shown.} \label{fig:E3}
\end{figure}
%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--

\subsection{Switchting the primary estimand (E4)}
The objective of the simulation study was to evaluate overall predictive
performance of \ainet{}. For this reason, we selected the Brier score as the
primary estimand, and subsequently determined the sample size to be able to
detect differences in the Brier score with sufficient accuracy. A range of
secondary estimates, such as the log-score, area under the curve (AUC),
calibration slope, and calibration in the large, and accuracy were computed.
Since the planned simulations did not show significant improvements in Brier
score of \ainet{} compared to the other methods, we looked at the secondary
outcomes and found \dots \todo{look at calibration metrics}

\subsection{Altering the objective of the simulation study (R2)}
The objective of the simulation study was to evaluate the overall predictive
performance of \ainet{}, since among estimation, prediction, and attribution,
this is where the method could potentially lead to improvements. We did,
however, not find any improvements over comparator methods, so one possibility
is to look for an entire differnet objective where \ainet{} performs better. For
instance, we could switch the focus on discrimination (\eg area under the curve)
or estimation performance (\eg mean squared error of the estimated
coefficients).
\todo{Find good setting. Figure}

\subsection{Handling exceptions differently (E6)}
In our protocol, we specified that it would be difficult to judge the results of
simulation settings in which more than 10\% of the runs fail to converge, and
that these settings will thus be omitted. However, without pre-specifying how
these situations should be handled, we could have tweaked the inclusion/exclusion
rules to find a rule that makes \ainet{} look better compared to other methods.
For instance, we could impute the missing scenarios with the ``worst-case''
observed performance of a method. In case of differential missingness (\ie one
method being more stable than another), this leads to biased comparisons. Thus,
for settings in which \ainet{} was more stable than its competitors, this
imputation scheme yields results unreasonably in favor of \ainet. Moreover, the
resulting confidence intervals ignore the imputation uncertainty and thus
underestimate $p$-values. \todo{Find good setting. Figure}

\subsection{Tweaking the number of simulations (E5)}

The number of simulations was chosen to obtain the desired precision for the
estimated mean Brier score of \ainet{}. Since the simulation did not indicate
any improvements in Brier, we could have reduced the number of simulations and
repeat it for different random seeds until one seed is found for which the
results look more favorable. \todo{Find good setting. Figure}

\section{Recommendations}

The previous sections painted a rather negative picture on how undisclosed QRPs
increase the risk of overoptimistic conclusions from simulation studies. In the
following, we summarize what we consider to be practical recommendations for
improving the methodological quality of simulation studies; see
Table~\ref{table:recommendations} for an overview. Our recommendations are
grouped with regards to which stakeholder they concern.

\begin{table}[!htb]
  \caption{Recommendations to improve rigour of comparative simulation studies
    and prevent QRPs.}
  \label{table:recommendations}
  \centering
	\begin{longtable}[b]{p{.95\textwidth}}
		\toprule
    \textit{Researchers} \\
    \midrule
    Adopt (preregistered) simulation protocols \\
    Collaboration between different research groups (with possibly
    ``competing'' methods) \\
    Blinded analyses of simulation results \\
    Distinguish exploratory from confirmatory findings
    \\[1em]

    \textit{Editors and reviewers} \\
    \midrule
    Require/demand (preregistered) simulation protocols \\
    Provide enough space for description of simulation methodology
    \\[1em]


    \textit{Journals and funders} \\
    \midrule
    Provide incentives for rigorous simulation studies (\eg badges on
    papers) \\
    Adopt reporting guidelines \\
    Adopting reproducibility checks \\
    Promote/fund research and software to improve simulation study methodology  \\[1em]

		\bottomrule
	\end{longtable}
\end{table}

\subsection{Recommendations for researchers}
Arguably the most important measure that researchers can take to prevent
themselves from engaging in QRPs is to adopt preregistration of simulation
protocols. This enables research consumers to distinguish between confirmatory
and exploratory findings, and it lowers the risk of potentially flawed methods
being promoted as improvement. While simulations protocols may at first seem 
a disadvantage due to additional work and possibly lower chance of publication,
they also provide researchers with a mean to think harder about setting up a
good simulation study, and thus differentiate themselves from unregistered
simulations.

Another way of improving simulation studies is to collaborate with other
researcher groups, possibly groups with ``competing'' methods. This helps to
design simulation studies which are more objective and whose results are more
useful for making a decision about which method to choose. To further reduce
subconscious biases researchers can also use blinding in the analysis of the
simulation results \citep{Dutilh2019}. For instance, they might shuffle the
method labels and only unblind themselves after the necessary analysis pipelines
are set in place. This approach also helps making \emph{post hoc} modifications
of the simulation study more legitimate. Finally, it is important for
researchers to separate exploratory from confirmatory findings in the reporting
of their simulation studies. By clearly indicating exploratory findings,
overoptimism can be avoided and lead to a more realistic picture.


\subsection{Recommendations for editors and reviewers}

By requiring higher methodological standards from simulation studies, reviewers
and editors help to improve the quality of the published literature. For
instance, they may demand from authors to make simulations protocols and
computer code available alongside the manuscript. Moreover, by providing enough
space and encouraging authors to provide detailed descriptions of their
simulation studies, replicability of the simulations studies can be improved.
Finally, reviewers should not be satisfied with manuscript that show that a
method is uniformly superior, but they should also urge authors to find cases
where their method is inferior or edge cases where it breaks down entirely.


\subsection{Recommendations for journals and funders}
Journals and funders can improve on the status quo by either demanding stricter
requirements or by providing incentives for more rigorous simulation study
methodology. For example, journals can make simulation protocols mandatory for
all articles featuring a simulations study. A less extreme measure would be to
indicate with a badge whether an article contains a preregistered simulation
study. Such an approach rewards researchers who take the extra effort. Similar
initiatives have led to a large increase in the adoption of preregistered study
protocols in the field of psychology \citep{Kidwell2016}. Another measure could
to require standardized reporting of simulation study, \eg the ``ADEMP''
reporting guideline from \citet{Morris2019}. Journals may also employ dedicated
staff for reproducibility checks to ensure computational reproducibility of the
published simulation studies. Finally, journals and funders can promote or fund
research and software to improve simulation study methodology. For instance, a
journal might have special calls for papers on simulation methodology.
Similarly, a funder could have special grants dedicated to software development
that facilitates sound design, execution, and reporting of simulation studies
\citep[as, for example,][]{White2010, Gasparini2018, Chalmers2020}.



\section{Conclusions} \label{sec:discussion}

Researchers rarely engage in QRPs with malicious intent but because humans
interpret ambiguous information self-servingly, and because they are good in
finding reasonable justifications that match with their expectations and desires
\citep{Simmons2011}. It is easier to publish novel and superior methods
\citep{Boulesteix2015}, so methodological researchers will typically desire to
show the superiority of a method rather than to disclose its strength's and
weaknesses.

By using a multitude of QRPs we were able to present a flawed method as an
improvement over comparators.
% This show the high degree of flexibility that researchers have when conducting
% comparative simulation studies.
We hope that our illustration will increase awareness about the fragility of
findings from simulation studies, and the need for higher standards.


% Bibliography
% ======================================================================
\newpage
\bibliographystyle{apalikedoiurl}
\bibliography{bibliography}


% Appendix
% ======================================================================
% \begin{appendices}
% 
% \section{Appendix title}
% \label{appendix:xxxx}
% 
% \end{appendices}

\end{document}
