\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphics}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{textcomp}
\usepackage[sc]{mathpazo}
\usepackage{times}
\usepackage{doi} % automatic doi-links
\usepackage[round]{natbib} % bibliography
\usepackage{multirow} % multi rows and columns for tables
\usepackage{longtable} % tables that span several pages
\usepackage{booktabs} % nicer tables
\usepackage[toc, page]{appendix} % better appendices
\usepackage{nameref} % reference appendices with names
\usepackage{todonotes} 

\input{defs}

%% margins
%% ----------------------------------------------------------------------------
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 right=25mm,
 top=20mm,
 bottom=20mm,
 }

%% title, authors, affiliations, mail
%% ----------------------------------------------------------------------------
\newcommand\longtitle{Questionable Research Practices in Simulation Studies}
\newcommand\shorttitle{\longtitle}
\newcommand\subtitle{Simulation Protocol}
\newcommand\longauthors{Samuel Pawel, Lucas Kook}
\newcommand\shortauthors{\longauthors} % if longauthors too long, change here
\newcommand\affiliation{
  Epidemiology, Biostatistics and Prevention Institute (EBPI) \\
  Center for Reproducible Science (CRS) \\
  University of Zurich
}
\newcommand\mail{}
\title{
  \vspace{-2em}
  \textbf{\longtitle} \\
  \subtitle
}
\author{
  \textbf{\longauthors}
}
\date{\today} % don't forget to hard-code date when submitting to arXiv!

%% hyperref options
%% ----------------------------------------------------------------------------
\usepackage{hyperref}  
\hypersetup{
  bookmarksopen=true, 
  breaklinks=true,
  pdftitle={\shorttitle}, 
  pdfauthor={\shortauthors},
  pdfsubject={},
  pdfkeywords={},
  colorlinks=true,
  linkcolor=RoyalPurple,
  anchorcolor=black,
  citecolor=MidnightBlue,
  urlcolor=BrickRed,
}

%% Headers and footers
%% ----------------------------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\shorttitle}
\rhead{\shortauthors}

\begin{document}
\maketitle

% knitr options
% =============================================================================
<< "main-setup", include = FALSE >>=
library(knitr)
opts_chunk$set(fig.height = 4,
               echo = FALSE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE, 
               eval = TRUE)
@


\section{Introduction}
\label{sec:introduction}
The purpose of this protocol is to describe our \emph{a priori} plans for a
comprehensive simulation study to evaluate the statistical properties of the
adaptive importance elastic net (\ainet) method. We will follow the ADEMP
approach (aims, data-generating process, estimands, methods, performance
measures) from \citet{Morris2019}.

Upon writing of this document only some preliminary evaluations have been made:
After the authors came up with the method on Wednesday 28. July 2021, the method
was implemented in R and evaluated on the iris data set and a few simulated data
sets. These analyses showed that for particular choices of hyperparameters, the
method could sometimes lead to improved predictive performance compared to
standard and L1-penalized logistic regression. However, in many cases
performance was actually equal or worse.

\section{Aims}
\label{sec:aims}
The aim of this simulation study is to systematically study the predictive
performance of \ainet~ for a binary prediction task. The simulation conditions
should resemble typical conditions found in the development of prediction models
in biomedical research. In particular we want to evaluate the performance of
\ainet~ conditional on
\begin{itemize}
  \item low- and high-dimensional covariates
  \item (un-)correlated covariates
  \item small and large sample size
  \item sparse and dense regression coefficient vectors
  \item varying noise levels
\end{itemize}
\ainet~ will be compared to other (penalized) binary regression models
from the literature, namely
\begin{itemize}
  \item Binary logistic regression: the simplest and most popular method for
        binary prediction
  \item Elastic net: a generalization of LASSO and ridge regression, the most
        widely used penalized regression methods
  \item Adaptive LASSO: the most popular weighted penalized regression method
  \item Random forest: a popular, more flexible method, also related to \ainet~
        by the penalization weights
\end{itemize}
These cover a wide range of established methods with varying flexibility and
serve as a reasonable benchmark for \ainet. There are many more extensions of
the adaptive LASSO in the literature \citep[see \eg the review
by][]{Vidaurre2013}, however, most of them focus on variable selection and
estimation instead of prediction, which is why we restrict our focus only on the
four methods from above.

\section{Data-generating process}
\label{sec:dgp}
In each simulation $b = 1, \dots, B$ we generate a data set consisting of $n$
realizations, \ie $\{(\ry_i, \rx_i)\}_{i=1}^n$. A datum $(\rY, \rX)$ consists of
a binary outcome $\rY \in \{0, 1\}$ and $p$-dimensional covariate vector
$\rX \in \RR^p$. The binary outcomes are generated by
\begin{align}
  Y \given \rx &\sim \BD\left(\expit\left\{\beta_0 + \rx^\top\shiftparm\right\}\right) \\
\intertext{and the covariate vectors are generated by}
  \rX &\sim \ND_p\left(0, \Sigma\right).
\end{align}
The baseline prevalence, $\prev = \expit(\beta_0) = 0.1$, is fixed for all
simulations to allow comparison of evaluation measures between different
simulation conditions. The coefficient vector $\shiftparm$ consists of
$q \leq p$ non-zero entries, \ie $\eshiftparm_{1:q} \neq 0, \; q \leq p$, and
those are generated from
\begin{align}
  (\beta_1, \dots, \beta_q)^\top \sim \ND_q(0, \Id)
\end{align}
once per simulation. The chosen parameter values should resemble values
occurring in practice \todo{cite}, as well as in previous simulation studies
\todo{cite}. Finally, the simulation parameters are varied fully factorially in
the following way:

<< "simulation-parameters" >>=
n <- c(100, 500, 1000)
EPV <- c(20, 10, 1, 0.1, 0.05)
sparsity <- c("dense", "sparse")
prev <- 0.1
sigma2 <- c(0.5, 1, 2)
rho <- c(0, 0.5, 0.95)
simGrid <- expand.grid(n = n, EPV = EPV, sparsity = sparsity, prev = prev,
                       sigma2 = sigma2, rho = rho, stringsAsFactors = FALSE)
simGrid$p <- with(simGrid, ceiling(n*prev/EPV))
simGrid$q <- with(simGrid, ifelse(sparsity == "sparse", floor(sqrt(n/log(p))), p))
simGrid$q <- with(simGrid, ifelse(!is.finite(q), p, q))
p <- ceiling(n*prev/EPV)


simGrid <- expand.grid("n" = n, "p" = p, "sigma2" = sigma2)
@

\subsection*{Sample size}
$n \in \{\Sexpr{n}\}$
\todo{justify and expand}

\subsection*{Dimensionality}
Several low- and high-dimensionality scenarios in $p$ are considered. We will
define the dimensionality $p$ via the easier to interpret events per variable
(EPV) by $$p := \ceil{\frac{n \cdot \prev}{\EPV}}.$$ $\EPV \in \{\Sexpr{EPV}\}$
are chosen to cover scenarios with low- to high-dimensional covariates
\citep[\cf][]{vanSmeden2018}.

\subsection*{Sparsity in $\shiftparm$}
We consider sparse and dense simulation settings for $\shiftparm$. We choose
sparsity of $\shiftparm$ based on the consistency requirement for the LASSO
\citep[Ch. 2.4.2]{buhlmann2011statistics}, \ie
$$q = \floor{\sqrt{n/\log p}},$$ so $q$ will depend on the sample size
$n$ and the number of covariates $p$ ( determined via the EPV).
In the simulations where $\shiftparm$ is dense, we set $q = p$.

\subsection*{Collinearity in $\rX$}
 No / medium / large collinearity,
  $\Sigma_{ij} := \sigma^2 \rho^{\lvert i - j \rvert},$
  $i,j = 1, \dots, p$, $\rho \in \{\Sexpr{rho}\}$
\todo{justify and expand}

\subsection*{Covariate variation level $\sigma^{2}$}
Several values for the variation in the covariates will be considered. Note that
this will influence the magnitude of the effect sizes of non-zero covariates.
We choose $\sigma^{2} \in \{\Sexpr{sigma2}\}$.

\subsection*{Test data}
In order to test the predictive performance, we generate a test data set of
10000 data points in each simulation $b$.

\section{Estimands}
\label{sec:estimands}
We will estimate different quantities to evaluate overall predictive performance,
calibration, and discrimination, respectively.
\begin{itemize}
  \item Primary estimand: Mean Brier score on independently generated test data,
  $$\BS := B^{-1} \sum_{b=1}^B n^{-1} \sum_{i=1}^n (y_{ib} - \hat{y}_{ib})^2,$$
  where $\hat{y} := \widehat\Prob(Y = 1 \given \rx)$. Lower values
  indicate better predictive performance in terms of calibration and
  sharpness. Since the prevalence is not altered in the DGP,
  there is no need to normalize the Brier score between simulations
  \citep{steyerberg2019clinical}.
  \item Secondary estimands:
  \begin{itemize}
    \item Mean log-score on independently generated test data,
    $$\LS := - B^{-1} \sum_{b=1}^B n^{-1} \sum_{i=1}^n y_{ib} \log(\hat{y}_{ib})
    + (1 - y_{ib}) \log (1 - \hat{y}_{ib}),$$
    will be used as a secondary measure of overall predictive performance. Lower
    values indicate better predictive performance in terms of calibration and
    sharpness.
    \item Mean AUC on independently generated test data,
    $$\text{AUC} = B^{-1} \sum_{b=1}^B n^{-1} \widehat\Prob(Y_i > Y_j \given \rx_i, \rx_j),$$
    where $Y_i$ and $Y_j$ denote case and non-case, respectively. The AUC
    will be used as a measure of discrimination and values closer to one
    indicate better discriminative ability. We use \pkg{pROC} to estimate
    the AUC \citep{pkg:proc}.
    \item Calibration slope $\hat b$ on independently generated test data, from
          the model
    $$\logit\Ex[Y \given \rx^\top\hat\shiftparm] = a + (b - 1) (\hat\beta_0 + \rx^\top\hat\shiftparm)$$
    This measure will be used to assess calibration and deviations from
    $b_{0} = 0$ indicate miscalibration.
    \item Calibration in the large $\hat a$ on independently generated test
          data, from the model
    $$\logit\Ex[Y \given \rx^\top\hat\shiftparm] = a + \hat\beta_0 + \rx^\top\hat\shiftparm$$
    This measure will also be used to assess calibration and deviations from
    $a_{0} = 0$ indicate miscalibration.
  \end{itemize}
\end{itemize}

To facilitate comparison between simulation conditions, all estimands will also
be corrected by the oracale version of the estimand, \eg the Brier score will be
computed from the ground truth $\shiftparm$ and the simulated data $\rx$,
subsequently the oracle Brier score and will be subtracted from the estimated
Brier score.

\section{Methods}
\label{sec:methods}
% TODO: Fill in 

\subsection{\ainet}

$$ \tilde{\ell}() = \ell(.) + \aipen$$

where $\wvec$ are derived from a random forest variable importance measure $\IMP$ as

$$w_j := 1 - \left(\frac{\IMP_j}{\sum_{k=1}^p \IMP_k}\right)^\gamma,$$

where we assume $\IMP$ to be normalized as

$$\IMP = \widetilde\IMP - \min_j\{\IMP_j\},$$

or

$$\IMP_j = \max\{0, \widetilde\IMP_j\}.$$

Per default, we choose mean decrease in the Gini coefficient for $\widetilde\IMP$.
Hyperparameters of the random forest are not tuned, but kept at their default
values (\eg \code{mtry}, \code{ntree}).

\subsection{Benchmark methods}

\begin{itemize}
   \item Binary logistic regression (if applicable)
    $$\ell(.)$$
   \item Elastic net
    $$\ell(.) + \llpen$$
   \item Adaptive LASSO (using binary glm weights)
    $$\tilde{\ell}() = \ell(.) + \aipen.$$
    In case $p > n$, we estimate the glm weights using a ridge penalty, tuned
    via cross-validation.
   \item Random forest from \pkg{randomForest}.
\end{itemize}

\section{Performance measures}
\label{sec:performance}
The distribution of all estimands from Section~\ref{sec:estimands} will be
assessed visually with box- and violin-plots that are stratified by method and
simulation conditions. Moreover, we will also compute
\begin{itemize}
  \item Mean
  \item Median
  \item Standard deviations
  \item Interquartile range
  \item 95\% confidence intervals
\end{itemize}
for each of them.
% TODO: compute B for desired MC error

\subsection*{Determining number of simulation}
We want to determine the number of simulation $B$ such that the Monte-Carlo
standard error of the primary estimand, the mean Brier score $\BS$,
is sufficiently small. The variance of $\BS$ is given by

\section{Software}
\label{sec:software}

% Bibliography
% ======================================================================
\bibliographystyle{../apalikedoiurl}
\bibliography{../bibliography.bib}


% Appendix
% ======================================================================
% \begin{appendices}
% 
% \section{Appendix title}
% \label{appendix:xxxx}
% 
% \end{appendices}

\end{document}
