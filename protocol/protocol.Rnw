\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphics}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{textcomp}
\usepackage[sc]{mathpazo}
\usepackage{times}
\usepackage{doi} % automatic doi-links
\usepackage[round]{natbib} % bibliography
\usepackage{multirow} % multi rows and columns for tables
\usepackage{longtable} % tables that span several pages
\usepackage{booktabs} % nicer tables
\usepackage[toc, page]{appendix} % better appendices
\usepackage{nameref} % reference appendices with names
\usepackage{todonotes} 

\input{defs}

%% margins
%% ----------------------------------------------------------------------------
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 right=25mm,
 top=20mm,
 bottom=20mm,
 }

%% title, authors, affiliations, mail
%% ----------------------------------------------------------------------------
% \newcommand\longtitle{Questionable Research Practices in Simulation Studies}
\newcommand\longtitle{Pitfalls and Potentials in Simulation Studies}
\newcommand\shorttitle{\longtitle}
\newcommand\subtitle{Simulation Protocol}
\newcommand\longauthors{Samuel Pawel, Lucas Kook}
\newcommand\shortauthors{\longauthors} % if longauthors too long, change here
\newcommand\affiliation{
  Epidemiology, Biostatistics and Prevention Institute (EBPI) \\
  Center for Reproducible Science (CRS) \\
  University of Zurich
}
\newcommand\mail{}
\title{
  \vspace{-2em}
  \textbf{\longtitle} \\
  \subtitle
}
\author{
  \textbf{\longauthors}
}
\date{\today} % don't forget to hard-code date when submitting to arXiv!

%% hyperref options
%% ----------------------------------------------------------------------------
\usepackage{hyperref}  
\hypersetup{
  bookmarksopen=true, 
  breaklinks=true,
  pdftitle={\shorttitle}, 
  pdfauthor={\shortauthors},
  pdfsubject={},
  pdfkeywords={},
  colorlinks=true,
  linkcolor=RoyalPurple,
  anchorcolor=black,
  citecolor=MidnightBlue,
  urlcolor=BrickRed,
}

%% Headers and footers
%% ----------------------------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\shorttitle}
\rhead{\shortauthors}

\begin{document}
\maketitle

% knitr options
% =============================================================================
<< "main-setup", include = FALSE >>=
library(knitr)
opts_chunk$set(fig.height = 4,
               echo = FALSE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE, 
               eval = TRUE)
@


\section{Introduction}
\label{sec:introduction}
The purpose of this protocol is to describe our \emph{a priori} plans for a
comprehensive simulation study to evaluate the statistical properties of the
adaptive importance elastic net (\ainet) method. We will follow the ADEMP
approach (aims, data-generating process, estimands, methods, performance
measures) from \citet{Morris2019}. We try to plan the simulation study as
thoroughly as possible. However, in practice there may be always difficulities
and unexpected discoveries which may warrant modifications. We will clearly label
these as exploratory findings in the analysis.

\section{Timeline}
\label{sec:timeline}
Upon writing of this document only some preliminary evaluations have been made:
After the authors came up with the method on Wednesday 28 July 2021, the method
was implemented in \textsf{R} and evaluated on the iris data set and a few 
simulated data sets.
These analyses showed that for particular choices of hyperparameters, the
method could sometimes lead to improved predictive performance compared to
standard and L1-penalized logistic regression. However, in many cases
performance was actually equal or worse.

We then started to write the simulation protocol. All versions of the protocol are
available on the GitHub repository (\url{https://github.com/SamCH93/SimPaper}),
the final version will be time\-stamped and tagged. As discussed in
Section~\ref{sec:performance}, some preliminary simulation runs will be conducted
to estimate the number of simulations needed to ensure a sufficiently small Monte
Carlo error. Also this milestone will be time\-stamped.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Aims} \label{sec:aims}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The aim of this simulation study is to systematically study the predictive
performance of \ainet{} for a binary prediction task. The simulation conditions
should resemble typical conditions found in the development of prediction models
in biomedical research. In particular we want to evaluate the performance of
\ainet{} conditional on
\begin{itemize}
  \item low- and high-dimensional covariates
  \item (un-)correlated covariates
  \item small and large sample sizes
  \item sparse and dense regression coefficient vectors
  \item varying baseline prevalences
\end{itemize}
\ainet{} will be compared to other (penalized) binary regression models
from the literature, namely
\begin{itemize}
  \item Binary logistic regression: the simplest and most popular method for
        binary prediction
  \item Elastic net: a generalization of LASSO and ridge regression, the most
        widely used penalized regression methods
  \item Adaptive LASSO: the most popular weighted penalized regression method
  \item Random forest: a popular, more flexible method. This method is related to
        \ainet{}, see Section~\ref{sec:methods}.
\end{itemize}
These cover a wide range of established methods with varying flexibility and
serve as a reasonable benchmark for \ainet. There are many more extensions of
the adaptive LASSO in the literature \citep[see \eg the review by][]{Vidaurre2013},
however, most of them focus on variable selection and
estimation instead of prediction, which is why we restrict our focus only on the
four methods from above.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data-generating process} \label{sec:dgp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
<< "simulation-parameters" >>=
## values for simulation
n <- c(100, 500, 1000, 5000)
EPV <- c(20, 10, 1, 0.1, 0.05)
sparsity <- c("dense", "sparse")
prev <- c(0.01, 0.05, 0.1)
sigma2 <- c(1)
rho <- c(0, 0.3, 0.6, 0.95)
simGrid <- expand.grid(n = n, EPV = EPV, sparsity = sparsity, prev = prev,
                       sigma2 = sigma2, rho = rho, stringsAsFactors = FALSE)
simGrid$p <- with(simGrid, ceiling(n*prev/EPV))
simGrid$q <- with(simGrid, ifelse(sparsity == "sparse", floor(sqrt(n/log(p))), p))
simGrid$q <- with(simGrid, ifelse(!is.finite(q), p, 0))
nScenarios <- nrow(simGrid)
@
In each simulation $b = 1, \dots, B$ we generate a data set consisting of $n$
realizations, \ie $\{(\ry_i, \rx_i)\}_{i=1}^n$. A datum $(\rY, \rX)$ consists of
a binary outcome $\rY \in \{0, 1\}$ and $p$-dimensional covariate vector
$\rX \in \RR^p$. The binary outcomes are generated by
\begin{align*}
  Y \given \rx &\sim \BD\left(\expit\left\{\beta_0 + \rx^\top\shiftparm\right\}\right) \\
  \intertext{with $\expit(z) = (1 + \exp(-z))^{-1}$ and the covariate vectors are generated by}
  \rX &\sim \ND_p\left(0, \Sigma\right).
\end{align*}
The baseline prevalence is $\prev = \expit(\beta_0)$. The coefficient vector
$\shiftparm$ contains $q \leq p$ non-zero entries at random indices and those
are generated from (with a slight abuse of notation)
\begin{align*}
  (\beta_1, \dots, \beta_q)^\top \sim \ND_q(0, \Id)
\end{align*}
once per simulation. This is a typical data-generating process for data with
binary outcomes and typically assumed in models for (high-dimensional) genetic
studies \citep{zhu2004} or clinical prediction problems
\citep{steyerberg2019clinical}. Finally, the simulation parameters are varied
fully factorially as described below, leading to a total of \Sexpr{nScenarios}
scenarios.



\subsection*{Sample size}
The sample size used in the development of predictions models varies widely
\citep{Damen2016}. We will use $n \in \{\Sexpr{n}\}$, which span typical values
occurring in practice. Note that previous simulation studies usually chose sample
size based on the implied number of events together with the number of covariates
in the model for easier interpretation \citep{vanSmeden2018, Riley2018}. We will use this approach in
reverse to determine the dimensionality of the parameters below.

\subsection*{Dimensionality}
Several low- and high-dimensionality scenarios in $p$ are considered. We will
define the dimensionality $p$ via events per variable
(EPV) by $$p = \ceil{\frac{n \cdot \prev}{\EPV}}$$
because previous simulation studies showed that $\EPV$ rather than the absolute
sample size $n$ influences
the predictive performance of a method. $\EPV \in \{\Sexpr{EPV}\}$
are chosen to cover scenarios with low- to high-dimensional covariates
\citep[\cf][]{vanSmeden2018}.

\subsection*{Sparsity in $\shiftparm$}
We consider sparse and dense simulation settings for $\shiftparm$. We choose
sparsity of $\shiftparm$ based on the consistency requirement for the LASSO
\citep[Ch. 2.4.2]{buhlmann2011statistics}, \ie
$$q = \begin{cases}
  \floor{\sqrt{n/\log p}} & p > 1 \\
  0 & p \leq 1,
\end{cases}$$
so $q$ will depend on the sample size
$n$ and the number of covariates $p$ ( determined via the EPV).
In the simulations where $\shiftparm$ is dense, we set $q = p$.

\subsection*{Collinearity in $\rX$}
We distinguish between no, low, medium and high collinearity. The diagonal
elements of $\Sigma$ are given by $\Sigma_{ii} = 1$ and the off-diagonal
elements are set to $\Sigma_{ij} = \rho$, $\rho \in \{\Sexpr{rho}\}$.
These values cover the typical (positive) range of correlations.

\subsection*{Baseline prevalence}
Different baseline prevalences $\expit(\beta_0) \in \{\Sexpr{prev}\}$ are 
considered, reflecting a reasonable range of prevalences for rare to
common diseases/adverse events.

\subsection*{Test data}
In order to test the out-of-sample predictive performance, we generate a test data set of
$n_{\text{test}} = 10000$ data points in each simulation $b$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Estimands} \label{sec:estimands}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We will estimate different quantities to evaluate overall predictive performance,
calibration, and discrimination, respectively. All methods will be evaluated on
independently generated test data.

\subsection{Primary estimand}

\begin{itemize}
  \item \textbf{Brier score.} We compute the Brier score as
  $$\BS = n_{\text{test}}^{-1} \sum_{i=1}^n (y_{ib} - \hat{y}_{ib})^2,$$
  where $\hat{y} = \widehat\Prob(Y = 1 \given \rx)$ and $B$ the number of simulations. Lower values
  indicate better predictive performance in terms of calibration and
  sharpness. A prediction is well-calibrated if the observed proportion
  of events is close to the underlying probabilities.
  The Brier score is a proper scoring rule, meaning that it is minimized if a 
  predicted distribution is equal to the data-generating distribution
  \citep{Gneiting2007}. It is thus a principled choice for our primary estimand.
\end{itemize}

\subsection{Secondary estimands}
\begin{itemize}
  \item \textbf{Scaled Brier score.} The scaled Brier score is computed as
  $$\BS^{*} = 1 - \BS/\BS_{0}$$
  with $\BS_{0} = \bar{y}(1 - \bar{y})$ and $\bar{y}$ the observed prevalence in
  the data set. The scaled Brier score takes into account that the
  prevalence varies across simulation conditions. Hence, the Brier score
  can be compared between conditions \citep{Schmid2005, steyerberg2019clinical}.

  \item \textbf{Log-score.} We compute the log-score on independently generated test data,
  $$\LS = - n_{\text{test}}^{-1} \sum_{i=1}^n \left\{ y_{ib} \log(\hat{y}_{ib})
  + (1 - y_{ib}) \log (1 - \hat{y}_{ib})\right\},$$
  will be used as a secondary measure of overall predictive performance. Lower
  values indicate better predictive performance in terms of calibration and
  sharpness. The log-score is a strictly proper scoring rule, however, it is more
  sensitive to extreme predicted probabilities compared to the Brier score.
  
  \item \textbf{AUC.} The AUC is given by
  $$\text{AUC} = \widehat\Prob(Y_i > Y_j \given \rx_i, \rx_j), \; i,j = 1, \dots, n_{\text{test}},$$
  where $Y_i$ and $Y_j$ denote case and non-case, respectively. The AUC is related
  to the area under the receiver-operating-characteristic (ROC) curve \citep{steyerberg2019clinical}.
  It will be used as a measure of discrimination and values closer to one
  indicate better discriminative ability. Disrimination describes the ability
  of a prediction model to discriminate between cases and non-cases.

  \item \textbf{Calibration slope $\hat b$.}  
  The calibration slope $\hat b$ is obtained by regressing
  the test data outcomes $y_{\text{test}}$ on the models' predicted logits $\logit({\hat{y}})$,
  \ie
  $$\logit\Ex[Y \given \hat\ry] = a + b\logit(\hat\ry).$$
  This measure will be used to assess calibration and deviations of $\hat b$ from
  one  indicate miscalibration.

  \item \textbf{Calibration in the large $\hat a$.} We inspect calibration in the
  large $\hat a$ on independently generated test data, from the model
  $$\logit\Ex[Y \given \hat\ry] = a + \logit(\hat{y})$$
  This measure will also be used to assess calibration and deviations of $\hat a$ from
  zero indicate miscalibration.
\end{itemize}

To facilitate comparison between simulation conditions, all estimands will also
be corrected by the oracale version of the estimand, \eg the Brier score will be
computed from the ground truth parameters and the simulated data $\rx$,
subsequently the oracle Brier score and will be subtracted from the estimated
Brier score.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods} \label{sec:methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{\ainet}

Choosing the vector of penalization weights in the adaptive LASSO becomes difficult
in high-dimensional settings. For instance, using absolute LASSO estimates as penalization
weights omits the importance of several predictors by not selecting them, 
especially in the case of highly correlated predictors \citep{Algamal2015}.
\ainet{} circumvents this problem by employing a random forest to estimate the 
penalization weights via an \emph{a priori} chosen variable importance measure.
This way, the importance of all variables enter the penalization weights simultaneously.

The penalized log-likelihood for \ainet{} for a single observation $(\ry, \rx)$
is defined as
$$\ell_{\text{AINET}}(\beta_0, \shiftparm; \ry, \rx, \alpha, \lambda, \wvec) = 
  \ell(\beta_0, \shiftparm; \ry, \rx) + \aipen$$
where
$$\ell(\beta_0, \shiftparm; \ry, \rx) = 
  \ry \log\left(\expit\left\{\beta_0 + \linpred\right\}\right)
  + (1 - \ry) \log\left(1 - \expit\left\{\beta_0 + \linpred\right\}\right)$$
denotes the log-likelihood of a binomial glm and
$\wvec$ is derived from a random forest variable importance measure $\IMP$ as
$$w_j = 1 - \left(\frac{\IMP_j}{\sum_{k=1}^p \IMP_k}\right)^\gamma,$$
where we transform $\IMP$ to be non-negative via
$$\IMP = \widetilde\IMP - \min_j\{\widetilde{\IMP}_j\},$$
or
$$\IMP_j = \max\{0, \widetilde\IMP_j\}.$$
\ainet{} is fitted by maximizing its penalized log-likelihood assuming i.i.d.
observations $\{(\ry_i, \rx_i)\}_{i=1}^n$, \ie
$$\arg\max_{\beta_0, \shiftparm} \sum_{i = 1}^n \ell_{\text{AINET}}(\beta_0,
\shiftparm; \ry_i, \rx_i, \alpha, \lambda, \wvec).$$

Per default, we choose mean decrease in the Gini coefficient for $\widetilde\IMP$.
Hyperparameters of the random forest are not tuned, but kept at their default
values (\eg \code{mtry}, \code{ntree}). The exponent $\gamma = 1$ will stay
constant for all simulations.

\subsection{Benchmark methods}

\begin{itemize}
   \item \textbf{Binary logistic regression} with and without ridge penalty for high- and
   low-dimensional settings, respectively. In case a ridge penalty is needed,
   it is tuned via 5-fold cross-validation.
   \item \textbf{Elastic net}, for which the penalized log-likelihood is given by
    $$\ell_{\text{EN}}(\beta_0, \shiftparm; \ry, \rx, \alpha, \lambda) = 
      \ell(\beta_0, \shiftparm; \ry, \rx) + \llpen.$$
    Here, $\alpha$ and $\lambda$ are tuned via 5-fold cross-validation.
   \item \textbf{Adaptive LASSO}, with penalized loss function
    $$\ell_{\text{adaptive}}(\beta_0, \shiftparm; \ry, \rx, \alpha, \lambda, \wvec)
    = \ell(\beta_0, \shiftparm; \ry, \rx) + \aipen.$$
    Here, the penalty weights $\wvec$ are inverse coefficient estimates from a
    binary logistic regression
    $$\hat{w}_j = \lvert\hat\eparm_j\rvert^{-\gamma},$$
    where $\lambda$ is tuned via 5-fold cross-validation.
    The exponent $\gamma = 1$ will stay constant for all simulations.
    In case $p > n$, we estimate the penalty weights using a LASSO penalty, tuned
    via an additional nested 5-fold cross-validation.
    \item \textbf{Random forest} for binary outcomes without hyperparameter tuning. The
    default parameters of \pkg{ranger} will be used \citep{ranger2017}. % , \ie
    % \code{ntree = 500}, \code{mtry = floor(sqrt(p))}, \code{min.node.size = },
    % \code{max.depth = }, \code{sample.fraction = }
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance measures} \label{sec:performance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The distribution of all estimands from Section~\ref{sec:estimands} will be
assessed visually with box- and violin-plots that are stratified by method and
simulation conditions. Moreover, to formally assess differences in predictive
performance between the methods we will regress the estimands on the method
and simulation conditions \citep[\cf][]{Skrondal2000}. We will also compute
\begin{itemize}
  \item Mean
  \item Median
  \item Standard deviations
  \item Interquartile range
  \item 95\% confidence intervals
\end{itemize}
for each of them.

\subsection{Determining the number of simulations}
We determine the number of simulation $B$ such that the Monte Carlo
standard error of the primary estimand, the mean Brier score $\BS$,
is sufficiently small. The variance of $\BS$ is given by
\begin{align*}
  \Var\left(\BS\right)
  &= B^{-1}n_{\text{test}}^{-1}\Var\left\{(y_{ib} - \hat{y}_{ib})^{2}\right\}
  % &= B^{-1}n_{\text{test}}^{-1} \left\{\Ex[(y_{ib} - \hat{y}_{ib})^{4}] -
  %   \Ex[(y_{ib} - \hat{y}_{ib})^{2}]^{2}\right\}
\end{align*}
and $\Var\left\{(y_{ib} - \hat{y}_{ib})^{2}\right\}$ could be decomposed
further \citep{Bradley2008}. However, the resulting expression is difficult to
evaluate for our data-generating process as it dependens on several of the
simulation parameters. We therefore follow a similar approach as in
\citet{Morris2019} and estimate
$\widehat{\Var}\left\{(y_{ib} - \hat{y}_{ib})^{2}\right\} < V$ from an initial
small simulation run to get an upper bound $V$ for worst-case variance across
all simulation conditions. The number of simulations is then given by
$$B = \frac{V}{n_{\text{test}} \Var\left(\BS\right)}.$$
Since $\BS \in [0, 0.5]$ we decide that we require the Monte Carlo standard
error of $\BS$ to be lower than $0.001$, hence we obtain
$$B =V/(n_{\text{test}} \cdot 0.001^{2}).$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Handling exceptions} \label{sec:exceptions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
It is inevitable that convergence issues and other problems will arise
in the simulation study, we will handle them as follow:
\begin{itemize}
  \item If a method fails to converge, the simulation will be excluded from the
  analysis
  \item We will report the proportion of simulations with convergence issues
  for each method
  \item In case of severe convergence issues or other problems (more than 10\% of the
  simulations failing within a setting), we may adjust
  the simulation parameters post hoc. This will be indicated in the discussion of
  the results.
  \item Convergence may be possible for certain tuning parameters of a method
  (\eg cross-validation of LASSO may fail for some values $\lambda$ while it could
  work for others). In this case we will choose a parameter value where the method
  still converges, as one would usually do with a real data set.

\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%y%%%%%%
\section{Software} \label{sec:software}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The simulation study is conducted in the \textsf{R} language for statistical
computing \citep{pkg:base} using the most recent version (at the time of writing
this protocol % \Sexpr{R.version$version.string}
R version 4.1.0).
\ainet{} is implemented in the \pkg{ainet} package and available on GitHub 
(\url{https://github.com/SamCH93/SimPaper}).
We use \pkg{pROC} to compute the AUC \citep{pkg:proc}. 
Random forests are fitted using \pkg{ranger} \citep{ranger2017}.
For penalized likelihood methods, we use \pkg{glmnet} \citep{Friedman2010,Simon2011} 
\todo{versions}.
The \pkg{SimDesign} package is used to set up simulation scenarios
\citep{Chalmers2020}.

% Bibliography
% ======================================================================
\bibliographystyle{../apalikedoiurl}
\bibliography{../bibliography.bib}


% Appendix
% ======================================================================
% \begin{appendices}
% 
% \section{Appendix title}
% \label{appendix:xxxx}
% 
% \end{appendices}

\end{document}
