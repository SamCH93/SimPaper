\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphics}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{textcomp}
\usepackage[sc]{mathpazo}
\usepackage{times}
\usepackage{doi} % automatic doi-links
\usepackage[round]{natbib} % bibliography
\usepackage{multirow} % multi rows and columns for tables
\usepackage{longtable} % tables that span several pages
\usepackage{booktabs} % nicer tables
\usepackage[toc, page]{appendix} % better appendices
\usepackage{nameref} % reference appendices with names
\usepackage{todonotes} 

\input{defs}

%% margins
%% ----------------------------------------------------------------------------
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 right=25mm,
 top=20mm,
 bottom=20mm,
 }

%% title, authors, affiliations, mail
%% ----------------------------------------------------------------------------
\newcommand\longtitle{Questionable Research Practices in Simulation Studies}
\newcommand\shorttitle{\longtitle}
\newcommand\subtitle{Simulation Protocol}
\newcommand\longauthors{Samuel Pawel, Lucas Kook}
\newcommand\shortauthors{\longauthors} % if longauthors too long, change here
\newcommand\affiliation{
  Epidemiology, Biostatistics and Prevention Institute (EBPI) \\
  Center for Reproducible Science (CRS) \\
  University of Zurich
}
\newcommand\mail{}
\title{
  \vspace{-2em}
  \textbf{\longtitle} \\
  \subtitle
}
\author{
  \textbf{\longauthors}
}
\date{\today} % don't forget to hard-code date when submitting to arXiv!

%% hyperref options
%% ----------------------------------------------------------------------------
\usepackage{hyperref}  
\hypersetup{
  bookmarksopen=true, 
  breaklinks=true,
  pdftitle={\shorttitle}, 
  pdfauthor={\shortauthors},
  pdfsubject={},
  pdfkeywords={},
  colorlinks=true,
  linkcolor=RoyalPurple,
  anchorcolor=black,
  citecolor=MidnightBlue,
  urlcolor=BrickRed,
}

%% Headers and footers
%% ----------------------------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\shorttitle}
\rhead{\shortauthors}

\begin{document}
\maketitle

% knitr options
% =============================================================================
<< "main-setup", include = FALSE >>=
library(knitr)
opts_chunk$set(fig.height = 4,
               echo = FALSE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE, 
               eval = TRUE)
@


\section{Introduction}
\label{sec:introduction}
The purpose of this protocol is to describe our \emph{a priori} plans for a
comprehensive simulation study to evaluate the statistical properties of the
adaptive importance elastic net (\ainet) method. We will follow the ADEMP
approach (aims, data-generating process, estimands, methods, performance
measures) from \citet{Morris2019}.

Upon writing of this document only some preliminary evaluations have been made:
After the authors came up with the method on Wednesday 28 July 2021, the method
was implemented in \textsf{R} and evaluated on the iris data set and a few 
simulated data sets.
These analyses showed that for particular choices of hyperparameters, the
method could sometimes lead to improved predictive performance compared to
standard and L1-penalized logistic regression. However, in many cases
performance was actually equal or worse.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Aims} \label{sec:aims}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The aim of this simulation study is to systematically study the predictive
performance of \ainet~ for a binary prediction task. The simulation conditions
should resemble typical conditions found in the development of prediction models
in biomedical research. In particular we want to evaluate the performance of
\ainet~ conditional on
\begin{itemize}
  \item low- and high-dimensional covariates
  \item (un-)correlated covariates
  \item small and large sample size
  \item sparse and dense regression coefficient vectors
  \item varying noise levels
\end{itemize}
\ainet~ will be compared to other (penalized) binary regression models
from the literature, namely
\begin{itemize}
  \item Binary logistic regression: the simplest and most popular method for
        binary prediction
  \item Elastic net: a generalization of LASSO and ridge regression, the most
        widely used penalized regression methods
  \item Adaptive LASSO: the most popular weighted penalized regression method
  \item Random forest: a popular, more flexible method, also related to \ainet~
        by the penalization weights
\end{itemize}
These cover a wide range of established methods with varying flexibility and
serve as a reasonable benchmark for \ainet. There are many more extensions of
the adaptive LASSO in the literature \citep[see \eg the review by][]{Vidaurre2013},
however, most of them focus on variable selection and
estimation instead of prediction, which is why we restrict our focus only on the
four methods from above.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data-generating process} \label{sec:dgp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In each simulation $b = 1, \dots, B$ we generate a data set consisting of $n$
realizations, \ie $\{(\ry_i, \rx_i)\}_{i=1}^n$. A datum $(\rY, \rX)$ consists of
a binary outcome $\rY \in \{0, 1\}$ and $p$-dimensional covariate vector
$\rX \in \RR^p$. The binary outcomes are generated by
\begin{align*}
  Y \given \rx &\sim \BD\left(\expit\left\{\beta_0 + \rx^\top\shiftparm\right\}\right) \\
\intertext{and the covariate vectors are generated by}
  \rX &\sim \ND_p\left(0, \Sigma\right).
\end{align*}
The baseline prevalence, $\prev = \expit(\beta_0) = 0.1$, is fixed for all
simulations to allow comparison of evaluation measures between different
simulation conditions. The coefficient vector $\shiftparm$ contains
$q \leq p$ non-zero entries at random indices and those are generated from
(with a slight abuse of notation)
\begin{align*}
  (\beta_1, \dots, \beta_q)^\top \sim \ND_q(0, \Id)
\end{align*}
once per simulation. The chosen parameter values should resemble values
occurring in practice \todo{cite}, as well as in previous simulation studies
\todo{cite}. Finally, the simulation parameters are varied fully factorially in
the following way:

<< "simulation-parameters" >>=
n <- c(100, 500, 1000)
EPV <- c(20, 10, 1, 0.1, 0.05)
sparsity <- c("dense", "sparse")
prev <- 0.1
sigma2 <- c(0.5, 1, 2)
rho <- c(0, 0.5, 0.95)
simGrid <- expand.grid(n = n, EPV = EPV, sparsity = sparsity, prev = prev,
                       sigma2 = sigma2, rho = rho, stringsAsFactors = FALSE)
simGrid$p <- with(simGrid, ceiling(n*prev/EPV))
simGrid$q <- with(simGrid, ifelse(sparsity == "sparse", floor(sqrt(n/log(p))), p))
simGrid$q <- with(simGrid, ifelse(!is.finite(q), p, q))
p <- ceiling(n*prev/EPV)


simGrid <- expand.grid("n" = n, "p" = p, "sigma2" = sigma2)
@

\subsection*{Sample size}
$n \in \{\Sexpr{n}\}$
\todo{justify and expand}

\subsection*{Dimensionality}
Several low- and high-dimensionality scenarios in $p$ are considered. We will
define the dimensionality $p$ via the easier to interpret events per variable
(EPV) by $$p := \ceil{\frac{n \cdot \prev}{\EPV}}.$$ $\EPV \in \{\Sexpr{EPV}\}$
are chosen to cover scenarios with low- to high-dimensional covariates
\citep[\cf][]{vanSmeden2018}.

\subsection*{Sparsity in $\shiftparm$}
We consider sparse and dense simulation settings for $\shiftparm$. We choose
sparsity of $\shiftparm$ based on the consistency requirement for the LASSO
\citep[Ch. 2.4.2]{buhlmann2011statistics}, \ie
$$q = \floor{\sqrt{n/\log p}},$$ so $q$ will depend on the sample size
$n$ and the number of covariates $p$ ( determined via the EPV).
In the simulations where $\shiftparm$ is dense, we set $q = p$.

\subsection*{Collinearity in $\rX$}
 No / medium / large collinearity,
  $\Sigma_{ij} := \sigma^2 \rho^{\lvert i - j \rvert},$
  $i,j = 1, \dots, p$, $\rho \in \{\Sexpr{rho}\}$
\todo{justify and expand}

\subsection*{Covariate variation level $\sigma^{2}$}
Several values for the variation in the covariates will be considered. Note that
this will influence the magnitude of the effect sizes of non-negative covariates.
We choose $\sigma^{2} \in \{\Sexpr{sigma2}\}$.

\subsection*{Test data}
In order to test the predictive performance, we generate a test data set of
10000 data points in each simulation $b$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Estimands} \label{sec:estimands}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We will estimate different quantities to evaluate overall predictive performance,
calibration, and discrimination, respectively.
\begin{itemize}
  \item Primary estimand: Mean Brier score on independently generated test data,
  $$\BS := B^{-1} \sum_{b=1}^B n^{-1} \sum_{i=1}^n (y_{ib} - \hat{y}_{ib})^2,$$
  where $\hat{y} := \widehat\Prob(Y = 1 \given \rx)$. Lower values
  indicate better predictive performance in terms of calibration and
  sharpness. Since the prevalence is not altered in the DGP,
  there is no need to normalize the Brier score between simulations
  \citep{steyerberg2019clinical}.
  \item Secondary estimands:
  \begin{itemize}
    \item Mean log-score on independently generated test data,
    $$\LS := - B^{-1} \sum_{b=1}^B n^{-1} \sum_{i=1}^n y_{ib} \log(\hat{y}_{ib})
    + (1 - y_{ib}) \log (1 - \hat{y}_{ib}),$$
    will be used as a secondary measure of overall predictive performance. Lower
    values indicate better predictive performance in terms of calibration and
    sharpness.
    \item Mean AUC on independently generated test data,
    $$\text{AUC} = B^{-1} \sum_{b=1}^B \widehat\Prob(Y_i > Y_j \given \rx_i, \rx_j),$$
    where $Y_i$ and $Y_j$ denote case and non-case, respectively. The AUC
    will be used as a measure of discrimination and values closer to one
    indicate better discriminative ability. 
    \item Calibration slope $\hat b$ on independently generated test data, from
          the model
    $$\logit\Ex[Y \given \rx^\top\hat\shiftparm] = a + (b - 1) (\hat\beta_0 + \rx^\top\hat\shiftparm)$$
    This measure will be used to assess calibration and deviations from
    zero indicate miscalibration.
    \item Calibration in the large $\hat a$ on independently generated test
          data, from the model
    $$\logit\Ex[Y \given \rx^\top\hat\shiftparm] = a + \hat\beta_0 + \rx^\top\hat\shiftparm$$
    This measure will also be used to assess calibration and deviations from
    zero indicate miscalibration.
  \end{itemize}
\end{itemize}

To facilitate comparison between simulation conditions, all estimands will also
be corrected by the oracale version of the estimand, \eg the Brier score will be
computed from the ground truth $\shiftparm$ and the simulated data $\rx$,
subsequently the oracle Brier score and will be subtracted from the estimated
Brier score.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods} \label{sec:methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{\ainet}

Choosing the vector of penalization weights in the adaptive LASSO becomes difficult
in high-dimensional settings. \ainet~ circumvents this problem by employing a
random forest to estimate the penalization weights via an \emph{a priori} chosen
variable importance measure.

The penalized log-likelihood for \ainet~ is defined as
$$\ell_{\text{AINET}}(\beta_0, \shiftparm; \ry, \rx, \alpha, \lambda, \wvec) = 
  \ell(\beta_0, \shiftparm; \ry, \rx) + \aipen$$
where
$$\ell(\beta_0, \shiftparm; \ry, \rx) = 
  \ry \log\left(\expit\left\{\beta_0 + \linpred\right\}\right)
  + (1 - \ry) \log\left(1 - \expit\left\{\beta_0 + \linpred\right\}\right)$$
denotes the log-likelihood of a binomial glm and
$\wvec$ is derived from a random forest variable importance measure $\IMP$ as
$$w_j := 1 - \left(\frac{\IMP_j}{\sum_{k=1}^p \IMP_k}\right)^\gamma,$$
where we transform $\IMP$ to be non-zero via
$$\IMP = \widetilde\IMP - \min_j\{\IMP_j\},$$
or
$$\IMP_j = \max\{0, \widetilde\IMP_j\}.$$

Per default, we choose mean decrease in the Gini coefficient for $\widetilde\IMP$.
Hyperparameters of the random forest are not tuned, but kept at their default
values (\eg \code{mtry}, \code{ntree}). The exponent $\gamma = 1$ will stay
constant for all simulations.

\subsection{Benchmark methods}

\begin{itemize}
   \item Binary logistic regression with and without ridge penalty for high- and
   low-dimensional settings, respectively. In case a ridge penalty is needed,
   it is tuned via cross-validation.
   \item Elastic net, for which the penalized log-likelihood is given by
    $$\ell_{\text{EN}}(\beta_0, \shiftparm; \ry, \rx, \alpha, \lambda) = 
      \ell(\beta_0, \shiftparm; \ry, \rx) + \llpen.$$
    Here, $\alpha$ and $\lambda$ are tuned via cross-validation.
   \item Adaptive LASSO, with penalized loss function
    $$\ell_{\text{adaptive}}(\beta_0, \shiftparm; \ry, \rx, \alpha, \lambda, \wvec)
    = \ell(\beta_0, \shiftparm; \ry, \rx) + \aipen.$$
    Here, the penalty weights $\wvec$ are inverse coefficient estimates from a
    binary logistic regression
    $$\hat{w}_j := \lvert\hat\eparm_j\rvert^{-\gamma},$$
    where $\lambda$ is tuned via cross-validation.
    The exponent $\gamma = 1$ will stay constant for all simulations.
    In case $p > n$, we estimate the penalty weights using a ridge penalty, tuned
    via an additional nested \todo{discuss} cross-validation.
   \item Random forest for binary outcomes without hyperparameter tuning.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance measures} \label{sec:performance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The distribution of all estimands from Section~\ref{sec:estimands} will be
assessed visually with box- and violin-plots that are stratified by method and
simulation conditions. Moreover, we will also compute
\begin{itemize}
  \item Mean
  \item Median
  \item Standard deviations
  \item Interquartile range
  \item 95\% confidence intervals
\end{itemize}
for each of them.
% TODO: compute B for desired MC error

\subsection*{Determining the number of simulations}
We determine the number of simulation $B$ such that the Monte-Carlo
standard error of the primary estimand, the mean Brier score $\BS$,
is sufficiently small. The variance of $\BS$ is given by
\begin{align*}
  \Var\left(\BS\right)
  &= B^{-1}n^{-1}\Var\left\{(y_{ib} - \hat{y}_{ib})^{2}\right\}
  % &= B^{-1}n^{-1} \left\{\Ex[(y_{ib} - \hat{y}_{ib})^{4}] -
  %   \Ex[(y_{ib} - \hat{y}_{ib})^{2}]^{2}\right\}
\end{align*}
and $\Var\left\{(y_{ib} - \hat{y}_{ib})^{2}\right\}$ could be decomposed even
further \citep{Bradley2008}. However, the resulting expression is difficult to
evaluate for our data-generating process as it dependens on several of the
simulation parameters. We therefore follow a similar approach as in
\citet{Morris2019} and estimate
$\widehat{\Var}\left\{(y_{ib} - \hat{y}_{ib})^{2}\right\} < V$ from an initial
small simulation run to get an upper bound $V$ for worst-case variance across
all simulation conditions. The number of simulations is then given by
$$B = \frac{V}{n \Var\left(\BS\right)}.$$
Since $\BS \in [0, 0.5]$ we decide that we require the Monte-Carlo standard
error of $\BS$ to be lower than $0.0001$, hence we obtain
$$B =V/(n \cdot 0.0001).$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Software} \label{sec:software}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The simulation study is conducted in the \textsf{R} language for statistical
computing \citep{pkg:base} \todo{version}. 
\ainet~ is implemented in the \pkg{ainet} package and available on GitHub 
\todo{link repository}.
We use \pkg{pROC} to compute the AUC \citep{pkg:proc}. 
Random forests are fitted using \pkg{randomForest}.
For penalized likelihood methods, we use \pkg{glmnet} \citep{Friedman2010,Simon2011} 
\todo{versions}.
The \pkg{SimDesign} package is used to set up simulation scenarios
\citep{Chalmers2020}.

% Bibliography
% ======================================================================
\bibliographystyle{../apalikedoiurl}
\bibliography{../bibliography.bib}


% Appendix
% ======================================================================
% \begin{appendices}
% 
% \section{Appendix title}
% \label{appendix:xxxx}
% 
% \end{appendices}

\end{document}
