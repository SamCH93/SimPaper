%% Template for a scientific paper by Samuel Pawel
%% Last modification: 17. December 2020
\documentclass[a4paper, 11pt]{article}
\usepackage{alltt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphics}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{textcomp}
\usepackage[sc]{mathpazo}
\usepackage{times}
\usepackage{doi} % automatic doi-links
\usepackage[round]{natbib} % bibliography
\usepackage{multirow} % multi rows and columns for tables
\usepackage{longtable} % tables that span several pages
\usepackage{booktabs} % nicer tables
\usepackage[title]{appendix} % better appendices
\usepackage{nameref} % reference appendices with names
\usepackage[onehalfspacing]{setspace}
\usepackage[labelfont=bf,font=small]{caption}
\usepackage{pdfpages}
\usepackage{pdflscape} 

%% margins
%% ----------------------------------------------------------------------------
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 right=25mm,
 top=20mm,
 bottom=20mm,
 }

%% title, authors, affiliations, mail
%% ----------------------------------------------------------------------------
\newcommand\longtitle{Pitfalls and Potentials in Simulation Studies}
\newcommand\shorttitle{\longtitle} % if longtitle too long, change here
\newcommand\subtitle{Questionable research practices in 
comparative simulation studies allow for spurious claims of superiority 
of any method}
\newcommand\longauthors{Samuel Pawel\footnote{Contributed equally.}
\footnote{Corresponding author: samuel.pawel@uzh.ch \newline 
Preprint. Version \today. Licensed under CC-BY.} , Lucas Kook$^*$, Kelly Reeve}
\newcommand\shortauthors{S. Pawel, L. Kook, K. Reeve} % if longauthors too long, change here
\newcommand\affiliation{
  \small Epidemiology, Biostatistics and Prevention Institute  \\
  \small Center for Reproducible Science \\
  \small University of Zurich,
  Hirschengraben 84, CH-8001 Zurich 
}
\title{
  \vspace{-2em}
  \textbf{\longtitle} \\
  \subtitle
}
\author{
  \textbf{\longauthors} \\
  \affiliation \\
  \small\{samuel.pawel, lucasheinrich.kook, kelly.reeve\}@uzh.ch
}
\date{} % \today} % don't forget to hard-code date when submitting to arXiv!

%% hyperref options
%% ----------------------------------------------------------------------------
\usepackage{hyperref}  
\hypersetup{
  bookmarksopen=true, 
  breaklinks=true,
  pdftitle={\shorttitle}, 
  pdfauthor={\shortauthors},
  pdfsubject={},
  pdfkeywords={},
  colorlinks=true,
  linkcolor=RoyalPurple,
  anchorcolor=black,
  citecolor=MidnightBlue,
  urlcolor=BrickRed,
}

%% Headers and footers
%% ----------------------------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\shorttitle}
\rhead{\shortauthors}

%% Useful commands
%% ----------------------------------------------------------------------------
%% Shortforms
% \newcommand{\eg}{\textit{e.g.,\,}} % e.g.
% \newcommand{\ie}{\textit{i.e.,\,}} % i.e.
% \newcommand{\cf}{\textit{cf\,}} % cf
% \newcommand{\vs}{\textit{vs\,}} % 
% \newcommand{\pkg}[1]{\textbf{#1}}
% \newcommand{\ainet}{\textsc{ainet}}
\input{defs.tex}
\newcommand{\Lucas}[1]{\textcolor{blue!80}{Lucas: #1}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle

% Abstract
% ======================================================================
\begin{center}
\begin{minipage}{13cm}
{\small
\rule{\textwidth}{0.5pt} \\
{\centering \textbf{Abstract} \\
Comparative simulation studies are workhorse tools for benchmarking statistical methods,
but if not performed transparently they may lead to 
overoptimistic or misleading conclusions. The current publication requirements adopted by 
statistics journals do not prevent questionable research practices such as selective 
reporting. The past years have witnessed 
numerous suggestions and initiatives to improve on these issues but little progress can be seen to date. In this 
paper we discuss common questionable research practices which undermine the validity 
of findings from comparative simulation studies. To illustrate our point, we invent 
a novel prediction method with no expected performance gain and benchmark it in a pre-registered
comparative simulation study. We show how easy it is to make the method 
appear superior over well-established competitor methods if no protocol is in place 
and various questionable research practices are employed. Finally, we provide researchers,
reviewers, and other academic stakeholders concrete suggestions for improving the 
methodological quality of comparative simulation studies, most importantly the
need for pre-registered simulation protocols.
}
\rule{\textwidth}{0.4pt} \\
\textit{Keywords}: 
benchmarking studies, Monte Carlo experiments, overoptimism, 
reproducibility, replicability, transparency
}
\end{minipage}
\end{center}

% Introduction
% ======================================================================
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
\begin{minipage}{12cm}
\emph{``The first principle is that you must not fool yourself and you are
the easiest person to fool. So you have to be very careful about that.
After you've not fooled yourself, it's easy not to fool other scientists.''}
\end{minipage}
\end{center}
\begin{flushright}
\citet[p.~12]{Feynman1974}
\end{flushright}

Simulation studies are to a statistician what experiments are to a scientist 
\citep{Hoaglin1975}. They have become a ubiquitous tool for the evaluation of 
statistical methods, mainly because simulation can be used to
solve problems which cannot be solved using purely theoretical arguments.
In this paper we focus on simulation studies where the objective is to compare
the performance of two or more statistical methods (\emph{comparative simulation
studies}). Such studies are needed to ensure that previously proposed methods
work as expected under various conditions, as well as to identify conditions under which
they fail. Moreover, evidence from comparative simulation studies is often the
only guidance available to data analysts for choosing from the plethora of
available methods \citep{Boulesteix2013, Boulesteix2017b}. Proper design and
execution of comparative simulation studies is therefore important, and the 
results of flawed studies can lead to serious damage due to misinformed decisions.

Just like empirical experiments in other fields of science, comparative simulation 
studies require many decisions to be made, for instance: How will the data be generated?
How often will a simulation condition be repeated? Which statistical methods will be 
compared and how are their parameters specified? How will the performance of the methods 
be evaluated? The degree of flexibility, however, is much higher for simulation
studies than for real experiments as they can often be rapidly repeated under different
conditions at practically no additional cost. This is why numerous guidelines
and best practices for design, execution, and reporting of simulation studies
have been proposed \citep{Hoaglin1975, Holford2000, Burton2006, Smith2010,
OKelly2016, Monks2018, Elofsson2019, Morris2019, Boulesteix2020B}. We
recommend \citet{Morris2019} for an introduction to state-of-the-art simulation
study methodology. 

Despite wide availability of such guidelines, statistics articles often 
provide too little detail about the reported simulation studies to enable
quality assessment and replication.
Journal policies sometimes require the computer code to reproduce the results,
but they rarely require or promote sound simulation methodology (\eg
the preparation of a simulation protocol). This leaves 
researchers with considerable flexibility in how they conduct and present
simulations studies. As a consequence, readers of statistics papers can
rarely be sure of the quality of evidence that a simulation study provides. 

Unfortunately, there are many questionable research practices (QRPs) which may
undermine the validity of comparative simulations studies and which can easily
go undetected under current standards. There is often a fine line between QRPs and
legitimate research practices. For instance, there are good reasons to modify the data-generating 
process of a simulation study based on the observed results, \eg if the initially 
considered data-generating process results in many missing or non-convergent simulations.
However, it is then important that such \emph{post hoc} modifications are transparently
reported. These practices only become questionable when they serve to confirm the hopes 
and beliefs of researchers regarding a particular method. Consequently, the results and 
conclusions of the study will be biased in favor of this method \citep{Niessl2021}.

% It is imperative to note, that in virtually all cases researchers, do not engage in QRPs with
% the intent to deceive, but due to their subconscious biases, expectations, or negligence
% citep{Simmons2011}. 
It is imperative to note that researchers most often do not engage in QRPs 
with the intent to deceive, but rather due to their subconscious biases,
expectations, or negligence \citep{Simmons2011}.
% Nevertheless, even perfectly honest researchers may unknowingly use 
% QRPs because there are many incentives to do so, \eg the pressure to publish novel and superior 
% methods \citep{Boulesteix2015}, or the urge to report simulation results concisely.
External pressures, \eg to publish novel and superior methods
\citep{Boulesteix2015} or to concisely report large amounts of simulation
results, may also lead honest researchers to (unknowingly) employ QRPs.
As we will argue, it is not only up to the researchers but also other academic stakeholders
to improve on these issues. 

The aim of this paper is to raise awareness about the issue of QRPs in comparative
simulation studies, and to highlight the need for the adoption of higher standards. 
To this end, we provide an illustrative list of QRPs
related to comparative simulation studies (Section \ref{sec:QRP}). With an
actual simulation study, we then show how easy it is to a present a novel,
made-up method as an improvement over others if QRPs are employed and 
\emph{a priori} simulation plans remain undisclosed (Section \ref{sec:study}).
The main inspiration for this work is drawn from similar illustrative studies which have
been conducted by \citet{Niessl2021} and \citet{Jelizarow2010} in the context
of benchmarking studies with real data sets and by
\citet{Simmons2011} in the context of $p$-hacking in psychological research.
In Section~\ref{sec:recommendations}, we then provide concrete
suggestions for researchers, reviewers, editors, and funding bodies to
to alleviate the issues of QRPs and improve the methodological quality of 
comparative simulation studies.
Section~\ref{sec:discussion} closes with a discussion of the results and 
concluding remarks.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Questionable research practices in comparative simulation studies} \label{sec:QRP} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There are various QRPs which threaten the validity of comparative simulation 
studies (see Table~\ref{table:QRPs} for an overview). QRPs can be categorized
with respect to the stage of research at which they can occur. Our
classification is inspired by the classification of QRPs in experimental
research from \citet{Wicherts2016}. In the following, we describe QRPs from all
phases of conducting a simulation study, namely, design, execution, and reporting.

\begin{table}[!htb]
  \caption{Types of questionable research practices (QRPs) in comparative simulation
  studies at different stages of the research process.}
  \label{table:QRPs}
  \centering
	\begin{tabular}{p{.05\textwidth} p{.1\textwidth} p{.75\textwidth}}
		\toprule
		\textbf{Tag} & \textbf{Related} & \textbf{Type of QRP} \\
  		\midrule
    \multicolumn{2}{p{.15\textwidth}}{\textit{Design}} & \\
  		% \cmidrule{1-1}
    D1 & R2 & Not/vaguely defining objectives of simulation study \\
    D2 & E1 & Not/vaguely defining data-generating process \\
  		%(data-generating process, parameters of the method, etc.) \\
    D3 & E2, E3 & Not/vaguely defining which methods will be compared and how their
                  parameters are specified \\
    D4 & E4 & Not/vaguely defining evaluation criteria \\
    D5 & E6 & Not/vaguely defining how to handle ambiguous analytic situations
              (\eg non-convergence of methods) \\
    D6 & E5 & Not computing required number of simulations to achieve
              desired precision \\[1em]
  
    \multicolumn{2}{p{.15\textwidth}}{\textit{Execution}} & \\
  		% \cmidrule{1-1}
    E1 & D2 & Adapting data-generating process to achieve certain outcomes \\
    E2 & D3, R1 & Adding/removing comparison methods based on outcome of simulations \\
    E3 & D3 & Selective tuning of hyperparameters of certain methods \\
    E4 & D4 & Choosing evaluation criteria based on outcome of simulations \\
    E5 & D5 & Choosing number of simulations to achieve desired outcome \\
    E6 & D6 & Including/excluding certain simulations to achieve desired outcome \\
    E7 & E5 & Choosing random seed to achieve desired outcome \\[1em]
  		
    \multicolumn{2}{p{.15\textwidth}}{\textit{Reporting}} & \\
  		% \cmidrule{1-1}
    R1 & E2, D3 & Selective reporting of results from simulation conditions
           that lead to certain outcomes \\
    R2 & D1 & Presenting exploratory simulation studies as confirmatory
              (HARKing) \\
    R3 & & Failing to report Monte Carlo uncertainty \\
    R4 & & Failing to assure computational reproducibility 
    (\eg not sharing code and
           details about computing environment) \\
    R5 & & Failing to assure replicability (\eg not reporting design and
           execution methodology) \\
		\bottomrule	
	\end{tabular}
\end{table}

\subsection{Design}
The \emph{a priori} specification of research hypotheses, study design, and
analytic choices is what separates \emph{confirmatory} from \emph{exploratory}
research \citep{Tukey1980}. Often, large comparative simulation studies follow 
the publication of a novel method (which is commonly proposed theoretically or 
together with a smaller, exploratory simulation study).
To allow readers to distinguish between confirmatory and exploratory
research, many non-methodological journals require pre-registration of study
design and analysis protocols. For instance, pre-registration is common 
practice in randomized controlled clinical trials \citep{DeAngelis2004}
and increasingly adopted in experimental psychology \citep{Nosek2018}.
Thus, if researchers plan to conduct confirmatory research, it is generally
recommended to write a simulation protocol \citep{Morris2019}. In contrast,
vaguely defined or undefined simulation study goals can be
considered a QRP. This is arguably even more important than in
non-methodological research because the multiplicity of design and analysis
choices in simulation
studies is far higher \citep{Hoffmann2021}. For instance, failing to define 
\emph{a priori}
the data-generating process (D2), the methods under investigation (D3),
or the evaluation metrics (D4) leaves a high number of \emph{researcher degrees
of freedom} open, which increases the chance of overoptimistic conclusions.

Another crucial part of rigorous design is simulation size calculation (see
Section~5.3 in \citet{Morris2019} for an overview). A thorough planning of the
number of simulations in terms of expected precision of the primary estimand is
important. While an arbitrarily chosen, often too small, number of simulations can be executed faster, they yield more heterogeneous results. By failing to
conduct a simulation size calculation (D6), researchers are at a higher risk
of drawing the wrong conclusions in the worst case 
(if their sample size is too small), or wasting
computer resources in the best case (if their sample size is too large).

\subsection{Execution}
While executing a simulation study, researchers can (often unknowingly) engage in 
various QRPs that increase the chance of showing superiority of their preferred method.
For instance, if under the originally envisioned data-generating process a proposed 
method does not perform better than its competitors, the data-generating process
may be adapted until conditions are found in which the proposed method appears superior (E1).
For example, noise levels, the number of predictors, or the effect sizes could be changed.
% For instance, if their favored method does not perform better than its competitors
% under the data-generating process the researcher initially had in mind,
% they can adapt the mechanism until they find conditions where their method is superior (E1).
% For example, they can change noise levels, the number of predictors, or
% the effect sizes.
It is usually not difficult to find reasonable justification for such modifications and 
then present them as if they were hypothesized during the planning of the study (R2).

In case no favorable data-generating processs can be found,
%under which a proposed method beats a competitor method, 
competitor methods that are superior to the proposed method may be excluded 
from the comparison altogether (E2). Similarly, additional methods which
perform worse under the (adapted) data-generating process may be included in
the simulation study.
% If researchers are unable to find any data-generating processs under which
% their favored method beats a competing method, they can exclude the competitor
% from the comparison altogether (E2). To make their favored method look better, additional
% methods which perform worse under the (adapted) data-generating process may
% also be included in the simulation study.

The methods under comparison may come with hyperparameters (\eg
regularization parameters in penalized regression models). In this case, 
hyperparameters of a favored method may be tuned until the method appears
superior to others (E3). Similarly, the hyperparameters of competitor methods
may be tuned selectively, \eg left at their default values.
% Researchers can tune the parameters of their favored method to achieve
% superiority over others (E3), while leaving the parameters of the competitor
% methods at their default values, or even actively searching for parameter values 
% which make the competitors look worse.

The evaluation criteria for comparing the performance of the investigated
methods can be changed to make a particular method look better than the others (E4). 
For instance, even though the original aim of the study may have been to compare
predictive performance among methods using the Brier score, the evaluation criterion
of the simulation study may be switched to area under the curve if the results 
suggest that the favored method performs better with respect to the latter metric.
This QRP parallels the well-known ``outcome-switching'' problem in clinical trials
\citep{Altman2017}.
% For instance, a researcher who initially wants to compare predictive
% performance of methods using the Brier score could switch to area under the
% curve if they realize that their method performs better with respect to the
% latter measure.
Similarly, the objective of the simulation study may also be changed depending on the 
outcome, \eg an initial comparison of predictive performance may be changed to comparing 
estimation performance if the results suggest that the favored method performs 
better at estimation tasks rather than prediction (see also R2).

If no \emph{a priori} simulation size calculation was conducted, the simulation size may
be changed until favorable results are obtained (E5).
% If no \emph{a priori} sample size calculation was conducted, researchers can tweak the
% number of simulations until they arrive at their desired conclusion (E5). 
If the number of simulations is small (relative to the noise level), the risk that a
positive finding is a false positive increases. 
Similarly, when too few simulations are conducted, the initializing seed for generating 
random numbers may be tuned until a seed is found for which a preferred method seems
superior (E7).
% Similarly, when too few
% simulations are conducted, researchers can ``tune'' the random numbers by
% changing the initialization seed of their random number generator until they
% % find a realization where their preferred method seems superior (E7).
% Examples of how random number sharing can be used to reduce Monte Carlo error
% in favor of a method are discussed in \citet{Morris2019}.

In some simulations, a method may fail to converge and thus produce missing values
in the estimands. If it is not pre-specified how these situations will be
handled, different inclusion/exclusion or imputation strategies may be tried out
until a favored method appears superior. Choosing an inadequate strategy can result 
in systematic bias and misleading conclusions.

\subsection{Reporting}
In the reporting stage, researchers are faced with the challenge of reporting
the design, results, and analyses of their simulation study in a digestible
manner. Various QRPs can occur at this stage. For instance, reporting may focus on results in which the method of interest performs best (R1).
% Researchers may selectively report results from simulation conditions in which
% their favored method performed best (R1). 
Failing to mention conditions in which
the method was inferior (or at least not superior) to competitors creates
overoptimistic impressions, and may lead readers to think that the method
uniformly outperforms competitors. 
Similarly, presenting exploratory simulation studies as if they were hypothesized
\emph{a priori} (R2) leads to over-confidence in the results.
% Similarly, when researchers conduct
% exploratory simulation studies but present their results as if they were
% hypothesized \emph{a priori} (R2), this leads to over-confidence in the method.

Failing to report Monte Carlo uncertainty (R3), \eg error bars or confidence
intervals reflecting uncertainty in the simulation, is a QRP with various
consequences \citep{van2019communicating}. It confounds the readers' ability to 
assess the quality of evidence from the simulation study. Furthermore, not
disclosing Monte Carlo uncertainty allows presenting random differences in
performance as if they were systematic.

Finally, by failing to assure computational reproducibility of the simulation study (R4),
it is more likely that coding errors remain undetected. By not
reporting the design and execution of the study in enough detail, other
researchers are unable to replicate and expand on the simulation study (R5).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical study: The Adaptive Importance Elastic Net
  (AINET)} \label{sec:study}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To illustrate the application of QRPs from Table~\ref{table:QRPs} we conducted
a simulation study. The objective of the study was to evaluate the predictive
performance of a made-up regression method termed the ``adaptive importance
elastic net'' (\ainet). The main idea of \ainet{} is to use variable
importance measures from a random forest for a weighted penalization of the
variables in an elastic net regression model. The hope is that this \emph{ad hoc}
modification improves predictive performance in clinical prediction modeling
settings where penalized regression models are frequently used. Superficially,
\ainet{} may seem sensible, however, for a linear data-generating process no
advantage over the classical elastic net is expected. For more details
on the method, we refer to the simulation protocol (Appendix~\ref{appendix:protocol}). 
We report the per-protocol simulation study results in
Appendix~\ref{appendix:per-protocol-results}. As expected, 
the performance of \ainet{} was virtually identical to standard elastic net
regression.  \ainet{} also did not yield any improvements over logistic
regression for the  data-generating process that we considered sensible 
\emph{a priori}.

%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--
\begin{figure}[!htb]
  \centering
  \includegraphics[width = 0.96\textwidth]{ainet-results.pdf}
  \caption{Differences in Brier score with 95\% adjusted confidence intervals
    between \ainet{} and random forest (RF), 
    logistic regression (GLM), elastic net (EN), and adaptive elastic net (AEN)
    are shown for representative simulation conditions (correlated covariates 
    $\rho = 0.95$, prevalence $\operatorname{prev} = 0.05$, a range of sample sizes
    $n$ and events per variable (EPV), in each simulation the Brier score is computed 
    for 10'000 test observations; for details see Appendix~\ref{appendix:protocol}).
    The top row depicts the per-protocol results in which \ainet{} does not
    outperform any competitor uniformly, except AEN. In the second row, we apply
    QRP E1: altering the data-generating process by adding a non-linear effect and 
    sparsity. The arrows point from the per-protocol result to the results under the
    tweaked simulation. In the third row, the QRP E2 is applied: EN is removed as
    a competitor. In the bottom row, selective reporting R1 is applied: only low 
    EPV settings are reported to give a more favorable impression for \ainet{}.
    Note, to reduce computation time, the most computationally expensive conditions
    with $n = 1'000$ and $\mbox{EPV} = 0.5$ were removed in the tweaked simulation.
    } \label{fig:E1}
\end{figure}
%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--%--

We now show how application of QRPs changes the above per-protocol conclusions.
Figure~\ref{fig:E1} illustrates different types of QRPs sequentially applied
to simulation-based evaluation of \ainet{}. The top row depicts the 
per-protocol differences in Brier score (x-axis) between \ainet{} and
competitor methods (y-axis) for a representative subset of the simulation
conditions. A negative difference indicates superior performance of \ainet{}.
In the second row, the arrows depict the change in the per-protocol
results after changing the data-generating process (E1). 
The third row shows the result after removal of the elastic net competitor 
(E2). Finally, the bottom row shows the end result where selective reporting 
of simulation conditions and competitor methods (R1) is applied to give 
a more favorable impression of \ainet{}.
We will now discuss these QRPs in more detail.

\paragraph{Altering the data-generating process (E1)}
We could not detect a systematic performance benefit
of \ainet{} over standard logistic regression, elastic net regression, or
random forest for the scenarios specified in the protocol. After simple
modifications of the data-generating process,
we found that 
\ainet{} outperforms logistic regression under the following conditions: 
only few variables being associated with the
outcome (sparsity), a non-linear effect, and a low number of events per
variable (EPV). Figure~\ref{fig:E1} (second row) shows the changes in Brier score 
difference between the pre-registered and the tweaked simulation. As can be seen, the
tweaked data-generating process leads to \ainet{} being superior to competitors 
in some conditions, and at least not inferior in others. 

\paragraph{Removing competitor methods (E2)}
Despite the adapted data-generating process, we still observed only minor (if any)
improvements of \ainet{} over the elastic net. In order to present \ainet{} 
in a better light we could omit the comparisons with the elastic net (E2),
as shown in Figure~\ref{fig:E1} (third row). 
This could be justified, for example, by arguing that a 
less flexible method (logistic regression), a more flexible 
method (random forest), and a comparably flexible method (adaptive elastic net)
are sufficient for neutral comparison.

\paragraph{Selective reporting of simulation results (R1)}
After the removal of the competitor elastic net, there are still
some simulation conditions under which \ainet{} is not superior to
the remaining competitors. To make \ainet{} appear more favorable, we may
thus report only simulation conditions with low EPV, as shown in 
Figure~\ref{fig:E1} (fourth row). 
This could be justified by the fact that journals require 
authors to be concise in their reporting. 
Moreover, further conditions with low EPV values could be simulated to 
make the results seem more exhaustive.
Focusing primarily on low EPV settings could be justified in 
hindsight by framing \ainet{} as a method designed for high-dimensional 
data (low sample size relative to the number of variables).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Recommendations}
\label{sec:recommendations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The previous sections painted a rather negative picture of how 
undisclosed changes in simulation design, analysis, and reporting
increase the risk of overoptimistic conclusions. In the
following, we summarize what we consider to be practical recommendations for
improving the methodological quality of simulation studies; see
Table~\ref{table:recommendations} for an overview. Our recommendations are
grouped with regards to which stakeholder they concern.

\begin{table}[!htb]
  \caption{Recommendations for improving quality of comparative simulation studies
    and preventing QRPs.}
  \label{table:recommendations}
  \centering
	\begin{tabular}{p{.95\textwidth}}
		\toprule
    \textit{Researchers} \\
    \midrule
    Adopt (pre-registered) simulation protocols \\
    Adopt good computational practices (code review, packaging, unit-tests, etc.) \\
    Blind analyses of simulation results \\
    Collaborate with other research groups (with possibly
    ``competing'' methods) \\
    Distinguish exploratory from confirmatory findings \\
    Disclose multiplicity and uncertainty of results \\[1em]

    \textit{Editors and reviewers} \\
    \midrule
    Require/demand conditions where methods break down \\
    Require/demand (pre-registered) simulation protocols \\
    Provide enough space for description of simulation methodology
    \\[1em]


    \textit{Journals and funding bodies} \\
    \midrule
    Provide incentives for rigorous simulation studies (\eg badges on
    papers) \\
    Require code and data \\
    Enforce adherence to reporting guidelines \\
    Adopt reproducibility checks \\
    Promote/fund research and software to improve simulation study methodology  \\[1em]

		\bottomrule
	\end{tabular}
\end{table}

\subsection{Recommendations for researchers}
Adopting pre-registrated simulation protocols is arguably the most important measure 
that researchers can take to prevent themselves from subconsciously engaging in QRPs. 
Pre-registration enables readers to distinguish between confirmatory
and exploratory findings, and it lowers the risk of potentially flawed methods
being promoted as an improvement over competitors. While pre-registered
simulation protocols may at 
first seem disadvantageous due to the additional work and possibly lower chance of publication,
they provide researchers with the means to differentiate their high-quality simulation
studies from the numerous unregistered and possibly untrustworthy simulation studies
in the literature. 
Platforms such as GitHub (\url{https://github.com/}), OSF 
(\url{https://osf.io/}), or Zenodo (\url{https://zenodo.org/}) can be used for
uploading and time-stamping documents.

When pre-registering and conducting simulation studies, we recommend using a
robust computational workflow. Such a workflow encompasses packaging the software, 
writing unit tests, and reviewing code \citep[see \eg][]{schwab2021statistical}. 
Other researchers and the authors themselves then benefit from improved computational
reproducibility and less error-prone code.

While planning a simulation study, it is impossible to think of all potential 
weaknesses or problems that may arise when conducting the planned simulations. 
In turn, researchers may be reluctant to tie their hands in a pre-registered protocol.
However, a transparently conducted and reported preliminary simulation can obviate most 
of these problems. We recommend researchers to disclose preliminary results and any ensuing 
changes to the protocol, \eg in a revised and time-stamped version of the protocol.
This approach parallels running a small pilot study, which is often done in empirical 
research.
A different approach for making \emph{post hoc} changes to the protocol is to use
blinding in the analysis of the simulation results \citep{Dutilh2019}. 
Blinded analysis is a standard procedure in particle physics to prevent
data analysts from biasing their result towards their own beliefs 
\citep{Klein2005}, and it lends 
legitimacy to post hoc modifications of the simulation study.
For instance, researchers might shuffle the method labels and only unblind themselves 
after the necessary analysis pipelines are set in place. 

Another way of improving simulation studies is to collaborate with other
researchers, possibly familiar with ``competing'' methods. This helps to
design simulation studies which are more objective and whose results are more
useful for making a decision about which method to choose. 

It is important for researchers to separate exploratory from confirmatory findings 
in the reporting of their simulation studies. By clearly indicating exploratory findings,
overoptimism can be avoided and and a more realistic picture of method performance
presented. Furthermore, researchers should try to disclose the multiplicity and
uncertainty inherent to the design and analysis of their simulation studies 
\citep{Hoffmann2021}. For instance, if possible, they should report sensitivity
analyses that show how much their conclusions change if different decisions 
would have been made. Methods from multivariate statistics can be used for
visualizing the influence of different design choices, \eg a multidimensional
unfolding approach as shown by \citet{Niessl2021}.

\subsection{Recommendations for editors and reviewers}
Peer review is an important tool for identifying QRPs in research results 
submitted to methodological journals. 
For instance, reviewers may demand researchers to include competitor 
methods which are not part of their comparison yet (or which might have been 
excluded from the comparison). However, reviewers can only identify a subset 
of all QRPs since some types are impossible to spot if no pre-registered 
simulation protocol is in place (\eg a reviewer cannot know whether the evaluation
criterion was switched). Even QRPs which can be detected by peer review
may be difficult to spot in practice.
It is thus important that reviewers and editors demand that authors make
simulation protocols and computer code available alongside the manuscript. 
Moreover, by providing enough space and encouraging authors to provide detailed 
descriptions of their simulation studies, replicability of the simulation 
studies can be improved. Finally, reviewers should not be satisfied with 
manuscripts showing that a method is uniformly superior; they should 
also urge authors to find cases where their method is inferior or edge 
cases where it breaks down entirely.

\subsection{Recommendations for journals and funding bodies}
Journals and funding bodies can improve on the status quo by either demanding stricter
requirements or by providing incentives for more rigorous simulation study
methodology. For example, journals can make (pre-registered) simulation protocols mandatory for
all articles featuring a simulations study. A less extreme measure would be to
indicate with a badge whether an article contains a pre-registered simulation
study. Such an approach rewards researchers who take the extra effort. Similar
initiatives have led to a large increase in the adoption of pre-registered study
protocols in the field of psychology \citep{Kidwell2016}. Another measure could be
to require standardized reporting of simulation studies, \eg the ``ADEMP''
reporting guideline by \citet{Morris2019}. Journals may also employ
reproducibility checks to ensure computational reproducibility of the
published simulation studies. This is already done, for example, by 
the Journal of Open Source Software or the Journal of Statistical
Software. Finally, journals and funding bodies can promote or fund
research and software to improve simulation study methodology. For instance, a
journal might have special calls for papers on simulation methodology.
Similarly, a funding body could have special grants dedicated to software development
that facilitates sound design, execution, and reporting of simulation studies
\citep[as, for example,][]{White2010, Gasparini2018, Chalmers2020}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions} \label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Pick up on the point that simulations are empirical studies of methods
Simulation studies should be viewed and treated analogously to (empirical)
experiments from other fields of science. The distinction between exploratory and
confirmatory simulation studies is essential to contextualize the results of
such a study. As in other empirical sciences, QRPs in simulation studies can 
obfuscate the usefulness of a novel method and lead to misleading and non-replicable results.

By deliberately using several QRPs we were able to present a method with no expected 
benefits and little theoretical justification -- invented solely for this article -- as an
improvement over theoretically and empirically well-established competitors.
While such intentional engagement in these practices is far from the norm, unintentional QRPs may have the same detrimental effect.
We hope that our illustration will increase awareness about the fragility of
findings from simulation studies and the need for higher standards.

While this article focused on comparative simulation studies, many of the issues
and recommendations also apply to neutral comparison studies with real data sets 
as discussed in \citet{Niessl2021}. Some of the noted problems even exist in 
theoretical research; due to the incentive to publish positive results, researchers
often selectively study optimality conditions of methods rather than conditions 
under which they fail.

Again, it is imperative to note that researchers rarely engage in QRPs with malicious 
intent but because humans tend to interpret ambiguous information self-servingly, and 
because they are good at finding reasonable justifications that match their expectations 
and desires \citep{Simmons2011}. As in other domains of science, it is easier to publish 
positive results in methodological research, \ie novel and superior methods
\citep{Boulesteix2015}. Thus, methodological 
researchers will typically desire to show the superiority of a method rather than 
to disclose its strengths and weaknesses. Aligning incentives for
individual researchers with rigorous simulation research will require a range of
actions involving various stakeholders in the research community. 
We have provided some recommendations that, we believe, could help achieve this goal.
Most importantly, we think that reviewers, journals, and funders need to raise 
the standards for simulation studies by requiring pre-registered simulation protocols
and rewarding researchers who invest the extra effort.

\section*{Software and data}
The simulation study was conducted in the \textsf{R} language for statistical
computing \citep{pkg:base} using the version 4.1.1. The method
\ainet{} is implemented in the \pkg{ainet} package and available on GitHub 
(\url{https://github.com/SamCH93/SimPaper}). We provide scripts for
reproducing the different simulation studies on the GitHub repository.
Due to the computational overhead, we also provide the resulting data 
so that the analyses can be conducted without rerunning the simulations.
We used \pkg{pROC} version 1.18.0 to compute the AUC \citep{pkg:proc}.
Random forests were fitted using \pkg{ranger} version 0.13.1 \citep{ranger2017}.
For penalized likelihood methods, we used \pkg{glmnet} version 4.1.2
\citep{Friedman2010,Simon2011}.
The \pkg{SimDesign} package version 2.7.1 was used to set up simulation scenarios
\citep{Chalmers2020}.

\section*{Acknowledgements}
We would like to thank Eva Furrer, Malgorzata Roos, and Torsten Hothorn for 
helpful discussion and comments on the simulation protocol and drafts of the
manuscript.
The authors declare that they do not have any conflicts of interest.

% Appendix
% ======================================================================
\begin{appendices}

\section{Simulation protocol}
\label{appendix:protocol}

Below, we include an excerpt of the final version of the protocol for the 
simulation-based evaluation of \ainet{}. All time-stamped versions
of the protocol are available at \url{https://doi.org/10.5281/zenodo.6364575}.

\input{simulation-protocol.tex}

\section{Per-protocol results}
\label{appendix:per-protocol-results}

\input{per-protocol-results.tex}

\end{appendices}

% Bibliography
% ======================================================================
\bibliographystyle{apalikedoiurl}
\bibliography{bibliography}

\end{document}

